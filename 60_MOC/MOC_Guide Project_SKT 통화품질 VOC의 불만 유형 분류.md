---
Created: 2025-02-09 16:57
tags:
---

# 개요
본 강의는 Transformer와 BERT 모델을 활용하여 SKT 통화품질 고객 상담 데이터(VOC)를 분류하는 방법을 다루는 자연어 처리 강의 내용을 정리한 것입니다. 딥러닝 기반 자연어 처리 기술이 실제 비즈니스 문제 해결에 어떻게 적용되는지 이해할 수 있습니다.

# 내용

## 1.  자연어 처리 개요

자연어 처리(NLP)는 사람이 사용하는 언어를 컴퓨터가 이해하고 처리할 수 있도록 하는 기술입니다. 자연어 처리는 크게 자연어 이해와 자연어 생성으로 나눌 수 있습니다. 이 기술은 최근 급속도로 발전하여 다양한 실생활 애플리케이션에 활용되고 있습니다:

- 번역 서비스 (예: 파파고)
- 챗봇
- 이메일 필터링
- 감성 분석

이러한 발전의 핵심에는 GPT와 같은 대용량 언어 모델이 있으며, 이들은 대량의 문서 데이터를 학습하여 다양한 목적으로 활용됩니다


## 2. 자연어 데이터의 전처리

#### 토큰화 (Tokenization)
- 토큰화 : 글을 일정한 단위인 토큰으로 나누는 작업
- 토큰 : 의미를 가지는 수준으로 상황에 따라 결정 
	- 예시)
		- 단어 기준: "나는 학교에 간다" → '나는', '학교에', '간다'
		- 형태소 기준: "나는 학교에 간다" → '나', '는', '학교', '에', '가', 'ㄴ다'
#### 정제 및 정규화
- 정제 : 필요하지 않거나 의미없는 단어를 제거하는 과정
- 정규화 : 같은 의미를 가지는 단어들을 통합하여 하나의 단어로 만드는 과정
	- 예시)
		- ‘합니다’, ‘하는 중입니다’, ‘하고 있습니다’ → ‘하다’로 통일
		- ‘Car’, ‘car’ → ‘car’로 통일
#### 표제어 추출 (Lemmatization)
- 표제가 되는 말을 추출하는 방법
- 다른 형태의 단어를 통합하여 의미를 유지하면서 간단하게 표현
	- 예시) ‘saw’ → ‘see’, ‘better’ → ‘good’
#### 어간 추출 (Stemming)
- 단어로부터 의미를 담고 있는 핵심 부분을 추출하는 방법
	- 예시) ‘running’ → ‘run’, ‘studies’ → ‘studi’
#### 불용어 (stopword) 처리
- 자주 등장하지만 문제를 푸는데 있어서 불필요한 단어인 불용어를 제거하는 가정
	- to, the

## 3. 자연어 데이터를 컴퓨터로 표현하기

### 임베딩
- 컴퓨터에서 처리하기 위해 숫자로 변환하는 과정이 필요
- 각 단어를 숫자 벡터와 연결시켜주는 다양한 방법이 존재하며,
- 이 숫자 벡터를 임베딩이라고 함

#### One-hot Encoding
- 표현하고 싶은 단어의 인덱스에 1, 나머지는 0을 부여하는 벡터 표현 방법
	- 예) 단어 집합: ‘hello’, ‘world’, ‘computer’, ‘AI’
		- ‘hello’ → 1, 0, 0, 0
		- ‘world’ → 0, 1, 0, 0
- 사용하는 단어 집합의 크기만큼의 차원이 필요함
- 단어의 정보가 벡터에 담기지 못하는 문제점이 발생함
- 단어들 간의 상관관계를 알 수 없음
#### Work2Vec
- 단어 벡터를 예측기반으로 학습하는 방법
- 단어 벡터를 무작위로 우선 설정하고,
- 각 텍스트의 단어 위치마다 각 단어의 주변 단어와 유사도를 계산하여 이 유사도가 높아지도록 벡터 학습 진행
- 하지만, 전체적인 문서의 특징을 반영 불가함
#### GloVe
- 예측 기반, 카운트 기반 방법론을 동시 사용하여,
- 중심 단어와 주변 단어 사이의 유사도 정보가 문서에 동시에 발견될 확률을 나타내는 정보가 될 수 있도록 학습방향 결정

#### 서브 워드(sub-word) 임베딩
- 단어도 더 쪼개어 서브 워드로 생각하여 학습
- 학습과정에서 쓰이지 않은 단어도 대처 가능
	- 예) ‘enjoy’ → ‘enj’, ‘joy’, ‘njoy’
- 장점: 새로운 단어가 나와도 처리 가능

## 4. Recurrent Neural Network (RNN)

#### RNN의 기본 원리
순환 신경망(Recurrent Neural Network, RNN)은 시퀀스 데이터를 처리하는 데 특화된 신경망 구조입니다. 자연어는 단어의 순서가 중요한 시퀀스 데이터이기 때문에 RNN과 잘 어울립니다.

#### RNN의 특징
- 자연어의 데이터 특성은 순환 신경망(RNN)과 특성이 잘어울림
- 입력과 출력을 시퀀스 단위로 처리하는 신경망 모델
- 다대일 또는 다대다 구조가 가능
- 이전 시점의 정보를 현재 계산에 반영
- 같은 가중치를 반복적으로 사용

## 5. RNN 기반 자연어 처리 모델

RNN은 입력 시퀀스가 길어질 경우 초기 정보가 소실되는 '장기 의존성 문제'를 가지고 있습니다. 이를 해결하기 위해 개발된 것이 LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit) 모델입니다.
#### RNN의 문제점
- 장기 의존성 문제가 있음. 극 초반에 입력된 정보가 잘 담기지 않을 수 있음
#### LSTM (Long Short-Term Memory)
- RNN의 장기 의존성 문제를 극복하기 위하여 LSTM 모델 도입
- LSTM의 주요 구성 요소:
	- 망각 게이트(f_t): 과거 정보를 얼마나 잊을지 결정
	- 입력 게이트(i_t): 현재 정보를 얼마나 기억할지 결정
	- 출력 게이트(o_t): 다음으로 넘겨줄 정보 조절
#### GRU (Gated Recurrent Unit)
: GRU는 LSTM을 간소화한 모델로, 셀 상태와 은닉 상태를 통합하고 게이트의 수를 줄여 계산 효율성을 높였습니다.
- LSTM의 역할을 유지하며 간소화한 모델
	- 셀 상태, 은닉 상태를 하나로 통합
	- 과거정보 기억, 현재정보 기억 방법을 하나로 통합
- 업데이트 게이트 : 과거/현재 정보 조절
- 리셋 게이트 : 과거 정보를 완전히 없앨지 결정

#### Encoder-Decoder 구조를 통한 번역
- 인코더(encoder) : 입력 언어 문장 정보 저장
- 디코더(decoder) : 입력 언어 문장 정보와 이전 시점 출력 단어 통하여 현재시점 출력 단어 생성

## 6. Attention 
: 기존 인코더-디코더 구조에서는 인코더의 마지막 은닉 상태에 모든 정보를 담아야 하는 한계가 있었습니다. Attention 메커니즘은 디코더가 출력을 생성할 때 인코더의 모든 은닉 상태를 참조할 수 있게 하여 이 문제를 해결했습니다.

### Attention이 도입된 동기 이해
- Attention이 추가된 RNN 기반 모델![[Pasted image 20250302160513.png]]
- Attention Output에 반영된 가중치를 활용![[Pasted image 20250302160904.png]]

### Attention의 핵심 계산 과정

1. 각 인코더 은닉 상태와 현재 디코더 은닉 상태 간의 유사도(Attention 점수) 계산
2. Softmax 함수를 통해 Attention 점수를 확률 분포로 변환
3. Attention 분포에 따라 인코더 은닉 상태를 가중합하여 Attention 출력값 생성


## 7. Transformer 와 Self Attention

### Transformer 모델
"Attention Is All You Need" 논문에서 소개된 Transformer는 RNN 없이 Attention만으로 시퀀스 모델링을 수행하는 혁신적인 구조입니다.

### Attention 과 Transformer
- 직렬 연산으로 인해 속도가 느리다는 문제점이 있음
- 기존 Attention
  ![[Pasted image 20250405152426.png]]

- Self-Attension : 같은 시퀀스 내 모든 위치 간의 관계를 고려
  ![[Pasted image 20250405152507.png]]

- Multi-Head Attention : 여러 개의 Attention을 병렬로 수행
  ![[Pasted image 20250405152825.png]]

- Feed-Forward 레이어: 표현력 강화
- Residual Connection과 Layer Normalization: 모델의 안정적 학습 지원
Positional Encoding: 위치 정보 제공

특히 Self-Attention은 시퀀스의 각 위치에 대해 Query, Key, Value를 계산하고, Query와 Key의 유사도를 기반으로 Value의 가중합을 구하는 방식으로 작동합니다.

## 8. BERT

BERT(Bidirectional Encoder Representations from Transformers)는 Transformer 인코더를 기반으로 한 사전 학습 언어 모델입니다. BERT는 다음 두 단계로 활용됩니다:
- 사전 학습(Pre-training): 대량의 텍스트 데이터로 일반적인 언어 이해 능력 학습
- 미세 조정(Fine-tuning): 특정 태스크에 맞게 모델 조정

### Pretrained Model
- 사전 훈련된 워드 임베딩
	- 사전 학습을 통하여 단어와 임베딩 벡터를 매칭시켜 단어에 대하여 의미있는 벡터를 가지도록 함
- 사전 훈련된 언어 모델
	- 단어 임베딩 학습에서 벗어나 모델 자체를 사전 훈련하여 사전 훈련된 모델의 가중치를 초기값으로 사용함
	- 모델이 언어에 대한 기본적인 이해를 할 수 있음
- ![[Pasted image 20250405160522.png]]

### BERT
- CLS : Special Classification Token
  ![[Pasted image 20250405160934.png]]

BERT의 주요 사전 학습 방법은 Masked Language Model(MLM)로, 입력 텍스트의 일부 단어를 MASK 토큰으로 대체하고 이를 예측하는 방식입니다. 구체적으로:
- 전체 단어의 15%를 무작위로 선택
- 그 중 80%는 MASK 로 변경, 10%는 무작위 단어로 변경, 10%는 그대로 유지

# 출처


# 관련 노트
[[[수업자료] 고객 상담 기록 데이터를 통한 문의 유형 분류.pdf]]


# 외부 링크

