---
Created: 2023-10-08 16:05
tags:
  - AI/머신러닝
share_link: https://share.note.sx/bnfqztz3#X//gmszCB4zWUeLqVG7zoKWGmWYQzyi0BaLVzSi9Kbs
share_updated: 2024-03-08T16:22:14+09:00
---

# 개요
빅데이터 분석기사 교재를 토대로 빅데이터 분석에 대한 전반적인 이론을 1차 정리 하였다. 
데이터 분석을 수행하기 위한 기획단계부터 탐색 > 모델링 > 검증까지 모든 내용이 망라되어 있다. 

***
# 내용

### 빅데이터 분석 기획
#### 빅데이터의 이해

##### 빅데이터 개요 및 활용
###### 데이터 관련 용어
- In-House DB : 기업 내부에서 자체 인력으로 DB를 구축하고 관리하는 것을 말함
- DW(Data Warehouse) : 사용자의 의사결정에 도움을 주기 위해 공통 형식으로 변환하여 관리하는 데이터베이스
- EDW(Enterprise Data Warehouse) : DW를 전사적으로 확장한 모델
- ERP(Enterprise Resource Planning) : 전사적 자원 관리를 위한 경영정보시스템의 한 종류
- EAI(Enterprise Application Integration) : 기업 응용 프로그램 통합
- OLAP(Online Analytical Processing) : 사용자가 다양한 각도에서 대화식으로 분석하는 과정, 데이터 웨어하우스이 데이터를 전략적인 정보로 변환하는 역할을 한다.
###### 빅데이터 경영혁신 단계
- 생산성 향상 > 발견에 의한 문제 해결 > 의사결정 향상 > 새로운 고객가치와 비지니스 창출
###### 빅데이터 산업의 발전
- 디지털화와 통합의 시대(2010년 이전) : DBMS, DW, ERP 기업 내 수행
- 분석과 연결의 시대(2010년 이후) : 데이터레이크로 진화, BI가 기업경영전략 수립에 중요하게 자리잡음
###### 빅데이터 분석과 인사이트
- 전략적인 인사이트를 가지고 핵심적인 비지니스에 집중하여 데이터를 분석하고 차별적인 전략으로 기업을 운영해야한다.
- 일차원적 분석(건설제조업) : 수요 예측, 재고 보충, 일정 관리, 수익 관리, 성과관리
###### [[데이터 사이언스]]
- 데이터 사이언스는 데이터로부터 의미 있는 정보를 추출해내는 학문
###### 데이터 분석 거버넌스 체계 수립
- 분석 준비도 평가
	- 분석 업무 파악, 분석 인력 및 조직, 분석 기법, 분석 데이터, 분석 문화, IT 인프라
- 분석 성숙도 평가
	- 1단계 (도입) : 분석시작, 환경과 시스템 구축
	- 2단계 (활용) : 분석결과를 업무에 적용
	- 3단계 (확산) : 전사 차원에서 분석 관리
	- 4단계 (최적화) : 분석을 진화시켜 혁신 및 성과 향상에 기여
- 결과진단
	- 크게 4가지 유형으로 분석 수준진단 결과를 구분할 수 있으며, 이에 대한 개선 방안을 수립 필요
###### 빅데이터 조직 및 인력 관리 방안
- 분석 조직 유형
	- 집중구조, 기능구조, 분산구조 3개가 있다.
	- (SKEE는 전사차원에서 지원할 수 있는 분산구조가 적합하다.)
- 분석 인력 구성
	- 비지니스 인력, IT 인력, 분석전문 인력, 변화관리 인력, 교육담당 인력 그리고 Director가 필요하지만,
	- 실질적으로 전사 도입이 아닌 이상 해당 조직구조로 운영되기 없기 때문에 TF형태로 운영되어야 한다.
- 분석과제 관리 프로세스
	- 과제발굴 : 분석 Idea 발굴 > 분석과제 후보 > 분석과제 확정
	- 과제수행 : 팀 구성 > 분석과제 실행 > 분석과제 진행관리 > 결과 공유/개선
##### 빅데이터 기술 및 제도
###### 빅데이터 플랫폼
- 빅데이터 플랫폼이란?
	- 데이터의 수집, 저장, 처리, 관리 및 분석 등의 역할 수행을 지원함으로써 새로운 인사이트와 비지니스 창출이 가능한 빅데이터 프로세스 환경을 의미
	- 빅데이터 플랫폼은 크게 아래와 같이 구분할 수 있다.
		- 빅데이터 관리 플랫폼 : 데이터를 수집, 저장, 처리, 관리를 담당
		- 빅데이터 분석 플랫폼 : 데이터 분석을 지원

###### 빅데이터와 인공지능
- 머신러닝과 딥러닝의 차이
	- 머신러닝 : 데이터를 분석하고 이를 바탕으로 결정을 내리기 위해 학습한 내용을 적용하는 알고리즘
	- 딥러닝 : 사람의 개입 없이 예측의 정확성 여부를 스스로 판단하고 결정

#### 데이터 분석 계획
##### 분석 방안 수립
###### 데이터 분석 로드맵 설정
- NCS 빅데이터 분석 기획 
	- NCS : National Competency Standard : 국가직무능력표준
	- 빅데이터 분석 기획의 능력 단위 요소
		- 도메인 이슈 도출하기 : 업무의 문제점 정의 및 빅데이터 분석을 통한 개선방향 도출
		- 분석 목표 수립하기 : 빅데이터 분석을 통해 얻고자 하는 목표 정의
		- 프로젝트 계획하기 : 빅데이터 분석을 위한 예산, 소요기간, IT환경 등을 고려한 WBS 작성
		- 보유 데이터 자산 확인하기 : 분석 목표와 프로젝트에 따른 사전 데이터 점검
- 분석 방법론
	- KDD 분석 방법론
	- CRISP-DM 분석 방법론 : KDD 분석보다 좀 더 세분화
	- [[빅데이터 분석 방법론(5단계)]]
		- (1단계) 분석 기획
		- (2단계) 데이터 준비
		- (3단계) 데이터 분석
		- (4단계) 시스템 구현
		- (5단계) 평가 및 전개 
- [[빅데이터 분석 과제 발굴 방법론]]
	- 하향식 접근법 : 분석대상을 알고 있을 경우 
	- 상향식 접근법 : 분석대상을 모르고 있을 경우
###### 데이터 분석 모델링
- [[데이터 분석 모델링 프로세스]]
	- 모델링을 위한 데이터 마트 설계와 구축
	- 탐색적 분석(EDA)과 유의 변수 도출
	- 모델링(모형화)
	- 모델링 성능 평가
	- 검증 및 테스트
	- 적용
##### 분석 작업 계획
###### [[NCS기반 빅데이터 분석 수행 절차]]
1. 빅데이터 분석요건 정의하기
2. 빅데이터 분석 데이터 확보 기획하기
3. 빅데이터 분석 데이터 탐색 기획하기
4. 빅데이터 분석 모델링 기획하기
5. 빅데이터 분석 결과 적용 계획하기
#### 데이터 수집 및 저장 계획
##### 데이터 수집
: 어떤 데이터를 수집할 것인지 원천데이터를 내,외부 시스템에서 탐색해서 결정한다.

###### 데이터 수집 방법
: 데이터 유형에 따라 수집방법을 나눌 수 있다.
- 정형 데이터 : DBMS, 스프레드시트 > ETL, FTP, Open API
	- ETL : 하나 이상의 데이터 소스로 부터 데이터 웨어하우스, 데이터 마트, 데이터 통합, 데이터 이동 등 다양한 응용시스템을 위한 데이터 구축에 필요한 핵심기술. 추출(Extract), 변환(Transform), 적재(Load)의 3단계 프로세스로 구성된다.
- 반정형 데이터 : HTML, XML, JSON > 웹 크롤링, RSS, Open API 
- 비정형 데이터 : 소셜 데이터, 문서, 이미지, 오디오 > 웹 크롤링, RSS, Open API

###### 데이터 변환
: 데이터 차원을 축소하고나 변수를 선택하여 데이터 마이닝이 효율적으로 적용되도록 데이터를 변경하는 조치를 말함
- 평활화 (Smoothing)
- 집계 (Aggregation) : 그룹화하여 연산하여 결과값을 요약
- 일반화 (Generalization) : 데이터를 특정 범위 내의 값으로 축소
- [[정규화 (Normalization)|정규화]] (Normalization) : 데이터의 이상값을 특정 범위 내로 변환하는 방법
	- 방법 : 최소-최대정규화, z-score 정규화, 소수 스케일링 정규화
- 범주화 : 상위 개념으로 통합
- 데이터 축소, 차원 축소 : 데이터 크기를 줄이거나 차원을 축소하여 데이터 셋을 다루기 쉽도록 변경
##### 데이터의 저장과 정리
###### 데이터 적재
- RDBMS : 관계혁 데이터베이스를 SQL로 사용하여 관리할 수 있는 소프트웨어. Oracle, MS SQL, MySQL 등이 해당
- NoSQL : Not Only SQL의 약자. 데이터 저장을 위한 스키마가 별도 필요없고 조인 연산을 미지원한다. MongoDB 등이 이에 해당한다.
- Scale-Out vs. Scale-Up
- 분산 파일 시스템 : 빅데이터를 확장 가능한 분산 파일 형태로 저장하는 방식이다.
###### 데이터 저장과 처리를 위한 플랫폼
- [[데이터 웨어하우스(Data Warehouse)]] : 데이터 베이스에 축적된 데이터를 공통의 형식으로 변환해서 관리하는 데이터 베이스를 말한다.
- [[데이터 레이크(Data Lake)]] : 방대한 데이터를 ETL과 DW구축의 한계에 다다랐고, 전통적인 데이터 뿐만 아니라 수많은 비정형 데이터를 실시간으로 수집, 정제, 통합, 분석해 활용하기 위해 '데이터레이크' 라는 새로운 개념의 플랫폼을 도입하고 있다.
###### 하둡 플랫폼에서의 데이터 저장과 처리
- 하둡 : 고가용성 분산 객체지향 플랫폼이다. 그리고 구글 GFS는 하둡의 분산파일시스템인 HDFS를 개발했다. (자세한 내용은 필요시 다루도록 하겠다.)
***
### 빅데이터 탐색
#### 데이터 전처리
##### 데이터 정제

###### 데이터 정제 과정
- 데이터 정제 단계에서 결측값, 이상값을 1차 처리한다.
- 분석 변수 처리 단계에서 변수 선택, 차원 축소, 파생변수 생성, 변수 변환 등 데이터 가공 및 변환 등을 2차 처리한다.
###### [[결측값 처리]] 
- 결측값에는 무작위 결측(MAR)과 비무작위 결측(NMAR)이 있으며, 비무작위 결측의 경우 결측값이 결과값에 영향을 주므로 데이터 정제 시 주의해야한다.
- 처리 방법은 삭제 또는 대체(보완)을 통하여 추정된 값으로 대체할 수 있다.
###### [[이상값 처리]]
- 이상값(이상치)란?
	- 데이터의 전처리 과정에서 발생 가능한 문제로 정상의 범주에서 벗어난 값을 의미한다.
- 이상값 검출 방법
	- 분산 : 정규분포의 97.5% 이상, 2.5%미만을 이상값이라 할 수 있다.
	- 우도 : 관측치가 가장 많이 발견될 것으로 보이는 경위의 확률값을 '우도'라고 하며, 우도가 낮을 수록 이상값에 가깝다고 할 수 있다.
	- 근접 이웃 기반 이상치 탐지(NN, KNN) : 정상값의 중심으로 부터 거리가 미리 정해진 임곗값보다 큰 경우 이상치로 본다.
	- 사분위수 : 일반적으로 사분범위에서 1.5 사분범위수(IQR)를 벗어나는 경우를 이상치로 판단한다. (Q1-1.5IQR, Q3+1.5IQR)
	- 그밖에도 밀도, 군집 등의 방법으로 이상치로 검출하는데, 분석기사에서는 사분위수를 가장 많이 채택하는 편이다.
- 이상값 처리 방법
	- 삭제, 대체, 스케일링, [[정규화 (Normalization)|정규화]] 등의 방법이 있다.
##### 분석 변수 처리
###### 변수 선택
- 종속변수에 유의미한 영향을 미칠 것으로 생각되는 '독립변수'를 선택하는 과정이다.
- 변수선택 방법에는 크게 아래와 같다.
	- 필터방법 : 전처리 과정 중에 불필요한 특징을 걸러내는 방법으로, 카이 스퀘어 검정통계량 등의 방법이 있다.
	- 래퍼방법 : 단계적으로 변수를 선택하는 방법이다.
		- 전진 선택법 : 독립변수 중에 가장 많은 영향을 줄 것 같은 변수부터 하나씨 추가하면서 모형을 선택하는 것
		- 후진 제거법 : 가장 적은 영향을 주는 변수부터 하나씩 제거하는 방법이다. F검정을 실시하여 p-value가 큰 변수부터 하나씩 제거해나간다. 
		- 단계적 방법 : 전진선택법과 후진제거법을 보완한 방법이다. 변수를 연속적으로 추가/제거해 나가면서 AIC가 낮아지는 모델을 찾는 법이다.
	- 임베디드 방법 : 알고리즘 자체에 변수 선택을 포함시키는 방법이다. 대표적으로 라쏘, 릿지, 엘리스틱넷 등이 있다.

###### 차원 축소
: 목적에 따라서 변수(데이터의 종류)의 양을 줄이는 것이다.
- PCA(주성분분석)
	- 차원 축소의 대표적인 알고리즘
	- 변수간에 존재하는 상관관계를 이용해 선형 연관성이 없는 저차원 공간으로 축소하는 방법
	- ![[빅데이터 주성분 분석(PCA)을 통한 차원축소.png]]
- LDA(선형판별분석)
	- PCA는 분산이 최대인 축을 찾는데 비해
	- LDA는 지도학습으로 데이터 분포를 학습하여 결정경계를 만들어 분석한다.
- 그밖에도 t-SNE, SVD, 비음수행렬 분해가 있다.

###### 파생변수 생성
- 기존 변수들을 조합하여 새롭게 만들어진 변수를 파생변수라 한다.
- 원데이터를 그대로 활용하기 보다 분석의 목표에 적합하게 계속해서 데이터 형태를 수정 및 보완할 필요가 있다.

###### 변수 변환
- 데이터를 분석하기 좋은 형태로 바꾸는 작업을 말하며, 불필요한 변수를 제거, 변환, 생성하여 분석을 효율적으로 하는데 도움을 준다.
- [[정규화 (Normalization)|정규화]] (스케일링) : 데이터의 이상값을 특정 범위 내로 변환하는 방법
- 표준화 : 데이터 분포가 한쪽으로 치우쳐 있을 경우(왜도가 심한 경우) 분포를 유사하게 변경하는 것을 말함

불균형 데이터 처리
- 어떤 데이터에서 각 클래스가 갖고 있는 데이터의 양에 차이가 큰 경우, 클래스 불균형이 있다고 말한다.
- 처리방법
	- 가중치 균형방법
	- 언더샘플링 : 대표 클래스의 일부만 선택하고, 소수클래스는 최대한 많은 데이터를 사용하는 방법
	- 오버샘플링 : 소수 클래스의 복사본을 만들어, 대표클래스의 수만큼 데이터를 만들어 주는 것
#### 데이터 탐색
##### 데이터 탐색 기초
###### 데이터 탐색(EDA)이란?
- 데이터를 이해하고 의미 있는 관계를 찾아내는 과정

###### 상관관계 분석
- 두 변수가 선형적 관계를 가지는지  분석하는 통계정 분석 방법
- 스피어만 상관계수, 피어슨 상관계수 등이 쓰인다.
	- 스피어만 상관계수 : 모수검정에 사용되며, 연속형 변수에 적용 가능하다.
	- 피어슨 상관계수 : 비모수검정에 사용되며, 이산형, 순서형 변수에 적용 가능하다
- 상관계수는 -1과 1사이의 값을 가지고 강도와 방향 측면에서 해석할 수 있다.

###### 기초통계량 추출
- 아래 값들과 같이 기초통계량을 추출하여 데이터의 특성을 빠르게 파악할 수 있다
	- 평균
	- 중앙값
	- 최빈값 : 주어진 데이터 중 가장 많이 나오는 값
	- 범위
	- 사분위수 범위(IQR) : 백분위수 기준으로 Q1(25%)와 Q3(75%)구간을 뺀 값을 말한다. 
	- 분산
	- 표준편차
	- 왜도 : 데이터 분포의 비대칭성을 나타내는 지표. 음수이면 왼쪽, 양수이면 오른쪽으로 비대칭된다.
	- 첨도 : 데이터가 중심에 어느정도 몰려있는가를 사용하는 척도이다. 첨도 값이 3이상이면 유의하게 벗어난 분포이다.

###### 시각적 탐색
- 히스토그램, 막대그래프, 줄기-잎 그림, 상자그림(box-plot), 산점도, 원그래프(pie chart) 등으로 탐색하는 단계이다.
##### 고급 데이터 탐색
###### 다변량 데이터 탐색
- 변수의 개수에  따라 일변량, 이변량, 다변량 분석으로 구분할 수 있다. 
- 상관분석 : 산점도 행렬을 그려 상관계수를 한 화면에서 확인할 수 있다.
- 다차원 척도법, 주성분분석, 선형판별분석 등을 활용하여 탐색이 가능하다.

###### 비정형 데이터 탐색
- 텍스트 마이닝 : 자연어 처리 기술 등을 활용해 텍스트 데이터를 수집하여 가공하여 탐색에 활용한다.
#### 통계기법 이해
##### 기술 통계
###### 데이터 요약
- 기술통계 : 표본 자체의 속성이나 특징을 파악하는 데 중점을 두는 데이터 분석 통계다.
- 자료(현상)의 변화 척도로는 범위, 편차, 분산, 표준편차가 있다.

###### 표본 추출
- 관련 용어
	- 모수 : 관심을 갖고 있는 모집단 관측치의 대표값. 모비율, 모평균, 모총계 등이 있다.
	- 표본 : 큰 데이터 집합에서 얻은 부분 데이터 집합
	- 모집단 : 어떤 데이터 집합을 구성하는 전체 대상 또는 전체 집합
	- 임의추출 : 무작위로 표본을 추출하는 것
- 표본 추출 방법
	- 확률 표본 추출 : 모집단 내의 모든 대상이 표본으로 선정확률을 동일하게 추출하는 방법
	- 비확률 표본 추출 : 비확률적인 방법으로 표본을 추출하는 방법으로, 모집단을 정확하게 규정지을 수 없을때 사용한다.

###### 확률 분포
- 확률분포 : 확률 변수가 특정한 값을 가질 확률을 나타내는 함수 또는 그래프로 나타내는 것을 말한다.
- 확률변수 : 결과를 예측할 수 없는 확률 실험에서 나타날 수 있는 확률적 결과를 수치로 표현한 값
	- 예를 들어 동전 앞뒷면을 0, 1로 부여할 수 있는데, 이렇게 부여된 실수값을 확률변수라 한다.

###### 표본 분포
- 표본분포 : 표본을 무수히 많이 추출한 후 추출된 표본들의 평균 값 혹은 표준편차를 통계량이라 할 수 있으며, 이 통계량의 분포를 표본분포라고 한다.
##### 추론 통계
- 추론 통계 : 통계학을 이용해 모집단의 특성을 추론하는 과정이다. 크게 추정과 가설검증으로 구분한다.

###### 점추정
- 모집단의 모수를 하나의 값을 추정하는 것이다.

###### 구간추정
- 모수가 포함될 것으로 기대되는 구간을 추정하는 것으로, 점추정과 달리 신뢰성 정도를 포함한다.
	- 90% 신뢰구간 z=1.645
	- 95% 신뢰구간 z=1.96
	- 99% 신뢰구간 z=2.576
###### [[가설검정]]
- 모집단 특성에 대한 주장 또는 가설을 세우고 표본에서 얻은 정보를 이용해 가설이 옳은지 판정하는 과정
- 개념
	- 귀무가설 : 실험을 통해 기각하고자 하는 어떤 가설로 H0으로 표시한다.
	- 대립가설 : 실험을 통해 증명하고자 하는 가설이며 H1 혹은 Ha로 표시한다.
	- 검정통계랑 : 가설 검정에 사용되는 표본 통계량으로 결론을 내릴때 사용하는 판단 기준이다.
	- 유의수준 : 귀무가설이 참인데도 이를 잘못 기각하는 오류를 범할 확률의 최대 허용한계로 1%, 5%(0.05)를 주로 사용한다.
	- 기각역 : 귀무가설을 기각하게 될 검정통계량의 영역
	- 채택역 : 귀무가설을 기각할 수 없는 검정통계량의 영역
	- 유의확률 : 귀무가설을 지지하는 정도를 나타낸 확률로 p-value라고도 함
- 오류
	- 제1종 오류 : 귀무가설이 참일때 귀무가설을 기각하는 오류를 의미한다.
	- 제2종 오류 : 귀무가설이 거짓일때 귀무가설을 채택하는 오류를 의미한다.
- 가설검정 예시
	- 단일 모평균 검정
	- 단일 모비율 검정
	- 단일표본 t 검정 : 모집단의 평균값을 특정 평균값과 비교하는 경우
	  ex) 전체 병아리의 평균 무게가 270인지?
	- 독립표본 t 검정 : 서로 다른 두 그룹의 평균 차이가 0인지 알아보는 검정 방법
	  ex) 남학생과 여학생의 하루 평균 운동 시간이 같은지?
	- 대응표본 t 검정 : 동일한 대상에 대해 두 가지 관측치가 있는 경우 이를 비교하여 검증하는 방법
	  ex) 운동 전후로 행복도에 차이가 있는지?


***
### 빅데이터 모델링
#### 분석 모형 설계

##### 분석 모형 선정/정의

###### 분석 모형 선정
- 분석 모형 선정 필요성
	- 분석 기법 또는 알고리즘을 적용하기 전에 분석 모형에 대한 선정이 필요하다.
	- 분석이 필요한 데이터 속성을 세부적으로 파악, 처리한 뒤에 분석 모형을 선정, 적합한 분석 기법을 선택하게 된다.
	- 데이터가 준비되어 있지 않다면 사전 분석 목적을 정확하게 파악해 분석 모형 선정을 수월하게 진행할 수 있다.
- 분석 모형 선정 프로세스
	1. 문제요건 정의 또는 비즈니스 이해에 따른 대상 데이터 선정과 분석 목표/조건 정의
	2. 데이터 수집 정리 및 도식화
	3. 데이터 전처리
	4. 최적의 분석 모형 선정

###### 분석 모형 정의
- 분석 모형 정의
	- 분석 모형은 분석 목표에 따라 데이터 특성을 도출하고, 가설 수립에 따라 전체적인 분석 방향성을 정의하는 모형이다.
- 분석 모형 종류
	- 에측 분석 모형 : "어떤 일들이 발생할 것인가?"
		- 날씨 예측, 주가 예측, 범죄/위험 예측 등에 등 과거~현재까지의 데이터와 상황에 따른 가설을 기반하여 미래에 대한 현상을 사전에 분류하고 예측하는 모형
	- 현황 진단 모형 : "과거에 어떤 상황이 왜 어떻게 일어났는가? 그리고 현재 어떠한 상태인가?"
		- 과거 데이터를 통해 현재 상황을 객관적으로 진단하는 모형으로 미래 예측이 아닌 현재를 이해함에 활용
	- 최적화 분석 모형 : "어떻게 하면 원하는 결과가 일어날 수 있을까?"
		- 제한된 자원, 환경 내에서 최대의 효용성, 이익과 같은 결과를 생성하기 위해 분석 모델을 최적화하는데 중점을 둔다.
- 분석 모형의 정의를 위한 사전 고려사항
	- 분석 모형을 정의하기 전에 분석이 실제로 추진될 수 있을지 가능성을 타진하는게 중요하다.
	- 접근 방법에는 상향식 접근과 하향식 접근이 있다. ([[빅데이터 분석 과제 발굴 방법론]])
		- 하향식 접근법 : 분석대상을 알고 있을 경우 
		- 상향식 접근법 : 분석대상을 모르고 있을 경우

###### 분석 모형 구축 방법
- 폭포수 모델
	- 고전적인 하향식 방법론으로, 프로세스가 단방향으로 진행되며 이전 단계가 완료되어야 다음 단계를 진행할 수 있다.
	- 프로세스 : 요구사항 분석 > 설계 > 구현 > 테스트 > 유지보수
- 프로토타이핑 모델
	- 폭포수 모델에서 피드백에 의한 반복이 어렵다는 점을 극복한 점진적인 프로세스 모델이다.
	- 우선적으로 초기 모델을 개발한 후 피드백을 통해 모든 요구사항이 반영될 때까지 보완하며 시스템을 완성한다.
	  ![[프로토타이핑 모델 프로세스.png]]
- 나선형 모델
	- 시스템 개발을 하면서 발생하는 위험을 최소화 하기 위해 개발 단계를 반복적으로 수행하며 점진적으로 완벽한 시스템으로 개발하는 모델이다.
	- 프로세스 : 목표설정 > 위험분석 > 개발/검증 > 고객평가/계획 > 다시 목표설정으로 순환
	- ![[나선형 모델 프로세스.png]]
 

##### 분석 모형 구축
###### 분석 시나리오 작성
- 시나리오 작성을 통해 분석 과정과 결과가 어떻게 활용되는지 명확하게 이해할 수 있다.
- 데이터 분석 대상 및 범위를 요구사항에 맞게 정의
- 분석을 통해 해결할 수 있는 문제와 목표, 그리고 분석 목표별 구현 모델과 예상결과를 작성한다.
- 분석 과정에 필요한 데이터, 절차, 분석기법 등의 세부사항들을 정의한다.
###### 분석 모형 설계
- 분석 대상 및 범위를 정하여 분석 목적을 구현하기 위한 분석 방법론을 설계
	-  분석 모형 설계 시 사전 확인 사항
		- 필요한 데이터 항목이 정해졌는가?
		- 데이터를 수집한 항목에 따라 단계별로 모델이 설계되었는가?
		- 분석 검증 통게 기법을 선정하였는가?
	-  분석 모델링 설계와 검정
		- 분석 목적에 기반한 가설검정 방법을 수립
		- 추정방법에 대한 기술을 검토
	-  분석 모델링에 적합한 알고리즘 설계
		- 비지도학습 : 군집분석, 연관성분석, 오토인코더 등
		- 지도학습 : 의사결정트리, 랜덤포레스트, 서포트벡터머신, 회귀분석 등
		- 준지도학습 : 셀프 트레이닝, 적대적 생성 모델
		- 강화학습 : Q-Learning
	-  분석 모형 개발 및 테스트
###### 분석 모델링 설계와 검정 - 분석 목적에 기반한 가설검정
- 유의수준 결정, 귀무가설과 대립가설 설정
- 검정통계량 설정
- 기각역의 설정
- 검정통계량 계산
	- 신뢰수준 : 가설을 검정할때 어느 정도로 검정할 것인지에 대한 수준
	- 유의수준 : 가설을 검정할떄 일정 수준을 벗어나면 귀무가설이 오류라고 판단
- 통계적인 의사결정(가설검정)
	- 계산한 검정 통계량을 t값 분포도와 비교하여 기각역에 속하는지 아닌지를 판단한다.

##### 분석 환경 구축
###### 분석 도구 선정
- 데이터 분석 도구
	- 엑셀 : 별도 명령어 없이 리본메뉴에서 회귀번석, 이동평균분석까지 가능하다.
	- R : 데이터 마이닝, 시각화 목적으로 개발된 프로그래밍 언어. 시각화가 용이하며 수많은 패키지가 개발되어 있음
	- 파이썬 : numpy, pandas, matplotlib 등의 라이브러리를 활용하여 통계분석이 가능하다. 
	- 하둡 : 분산 환경에서 빅데이터를 저장, 처리할 수 있는 자바 기반의 오픈소스 프레임워크. 구축 비용이 저렴하다
	- 그밖에 맵리듀스, SAS, SPSS, Stata 등이 있다.
###### 데이터 분할
- 데이터 분할
	- 전체 데이터를 학습/검증/테스트 데이터로 나누는 과정을 말한다.
	- 일반적으로 학습 80%, 테스트 20%로 분할하고, 
	- 또는 학습 60%, 검증 20%, 테스트 20% 로도 분할한다.
- 과적합 (ovefitting)
	- 학습데이터를 과하게 학습하는 것을 말한다. 
	- 과적합을 하게 되면 이미 학습한 훈련용 데이터는 성능이 높게 나오지만, 학습하지 않은 데이터는 성능이 낮게 나온다.
- 과소적합 (underfitting)
	- 너무 단순해서 학습데이터 조차 제대로 예측하지 못하는 경우를 말한다.
- 데이터 분할을 통한 검증 방법
	- 홀드아웃
		- 가장 보편적인 데이터 분할을 통한 검증방법
		- 랜덤하게 데이터를 추출해 학습/테스트 데이터로 분리하는 방식
	- k-폴드 교차검증
		- 무작위로 중복되지 않는 k개 데이터 셋으로 나눈 후 k-1개의 데이터를 학습 데이터로 서용하고 나머지 1개 데이터셋을 검증데이터로 활용하는 방식
	- [[부트스트랩(Bootstrap)|부트스트랩]]
	- startified k-폴드 교차검증

#### 분석기법 적용

##### 개요
: 데이터 분석 모델은 학습 유형에 따라 크게 지도학습, 비지도학습, 준지도학습, 강화학습으로 구분된다.
###### [[지도학습]]
- 정의
	- 정답이 있는 데이터를 활용해 분석 모델을 학습시키는 것을 말함
	- 예를 들어 동물 사진을 주고 해당 동물이 고양이 사진인지 아닌지 판단하게 수만번 학습시키는 것
- 유형
	- 분류에 사용되는 유형 (레이블이 범주형)
		- [[로지스틱 회귀분석]], 신경망모형, [[의사결정 트리 분석|의사결정나무]](분류트리모형), KNN, [[앙상블 분석]], [[서포트 벡터 머신(SVM)|SVM]] 등
	- 회귀에 사용되는 모형 (레이블이 연속형)
		- [[단순 선형 회귀분석|선형회귀분석]], [[의사결정 트리 분석|의사결정나무]](회귀트리모형), SVR, 신경망 모형, 릿지, 라쏘
###### [[비지도학습]]
- 정의
	- 정답을 알려주지 않고 학습시키는 방법
	- 정답 레이블이 없는 데이터를 비슷한 특징을 가진 데이터끼리 군집화해 새로운 데이터에 대한 결과를 예측
	- 예를 들어 각각 어떤 동물인지에 대한 레이블이 없이 비슷한 동물들이 특징을 기준으로 군집화 한다. 
- 유형
	- 군집에 사용되는 모형
		- K-means, SOM, DBSCAN, 병합군집, 계층군집
	- 차원축소에 사용되는 모형
		- PCA(주성분 분석), LDA(선형판별분석), SVD(특잇값분해), MDS(다차원척도법)
	- 연관분석에 사용되는 모형
		- Apriori

###### 준지도학습
- 정의
	- 정답이 있는 데이터와 없는 데이터를 동시에 학습에 사용하는 기법이다.
- 유형
	- 셀프 트레이닝 : 정답이 있는 데이터로 모델을 학습한 뒤 정답이 없는 데이터를 예측하여 높은 확률 값이 나오는데 가중치를 주는 진단 기법
	- 생성적 적대 신경망 : 생성모델과 판단모델이 존재하하여 데이터 분포 법칙에 따라 생성하고 판별하는 방식으로 학습을 진행한다.

###### 강화학습
- 정의
	- 주어진 환경에서 보상을 최대화 하도록 에이전트를 학습시키는 방법
	- 에이전트와 환경의 상태 등이 인공신경망에 들어가게 되며, 에이전트가 행동을 결정하고 환경을 통해 보상이 있으면 이전의 입력값과 해동을 긍정적으로 학습하게 된다.

##### 분석기법

![[데이터 분석 알고리즘 별 대표분야.png]]
###### 선형 회귀분석
- 개념
	- 독립변수와 종속변수 간에 선형적인 관계를 도출하여 독립변수가 종속변수에 미치는 영향을 분석하고, 독립변수를 통해 종속변수를 예측하는 분석기법
	- 유투브 시청시간이 학생에게 영향을 미치는 것을 분석한다면
		- 독립변수가 유투브 시청시간이 되고,
		- 종속변수가 학성의 성적이 된다.
- 알고리즘
	- 데이터 분포를 토대로 추세선을 정의하면 독립변수에 따른 종속변수 값을 예측할 수 있게 된다.
	- 최소 제곱법을 통해 파라미터를 추정하고 추정된 파라미터를 통해 추세선을 그려 값을 에측하는 것이 회귀분석의 기본 알고리즘이다.
- 종류
	- [[단순 선형 회귀분석|단순회귀분석]], [[다중 선형 회귀분석|다중회귀분석]], 다항회귀분석, 비선형회귀분석이 있다.
###### [[로지스틱 회귀분석]]
- 개념
	- 독립변수의 선형결합을 이용해 사건의 발생 가능성을 예측한다.
	- '어떤 사건이 발생할 확률 or 발생하지 않을 확률'을 예측하여 분류를 진행한다. 
	- 질병 유무와 같이 종속변수가 이항형 문제일 경우 사용할 수 있다. 보통 질병이 있는 경우를 '1', 없는 경우를 '0'으로 분석한다.
- 알고리즘
	- 오즈
		- 사건이 발생활 확률이 사건이 발생하지 않을 확률 의 몇배인지에 대한 개념이다.
		- 오즈비는 오즈의 비율로 오즈를 오즈로 나눔으로써 비교할 수 있다. 
			- 비흡연자에 대한 흡연자의 폐암 발병 오즈비를 구하면 4/0.25로 오즈비는 16이 되고, 
			- 이는 흡연자가 비흡연자 보다 폐암 발병 확률이 16배 높다고 해석할 수 있다.
	- 로짓변환
		- 오즈의 범위를 무한대로 변환함으로써 회귀분석을 적용할 수 있다.
	- 시그모이드 함수
		- 로짓함수와 역함수 관계이다.
###### [[의사결정 트리 분석]]
- 개념
	- 데이터를 학습하여 데이터 내에 존재하는 규칙을 찾아내고, 이 규칙을 나무구조로 모형화해 분류와 예측을 수행하는 방법이다.
	- 올바른 분류를 위해서 상위노드에서 하위노드로 갈수록 집단 내에서는 동질성을 가지고, 집단간에는 이질성이 커져야한다.
	- 중간 노드가 많다는 것은 규칙이 복잡하다는 이야기이므로, 모델이 과적합되기 쉽기 때문에 나무의 깊이를 적절하게 조절해야한다.
- 종류
	- 분류나무 (이산형 목표변수)
		- 이산형 목표변수에 따른 빈도 기반의 분리에 사용한다.
		- 상위노드에서 가지 분할을 진행할 떄 카이제곱 통계량의 P-value, 지니지수, 엔트로피지수 등이 분리 기준에 활용된다.
	- 희귀나무 (연속형 목표변수)
		- 연속형 목표변수에 따른 평균/표준편차 기반 분리에 사용하며 특정 의미를 지니는 실수값을 출력한다.
		- 상위노드에서 가지분할을 진행할때 F-통계량의 p-value, 분산의 감소량 등이 분리의 기준이 된다.
- 분석과정
	- 의사결정트리 분석 과정
		- 변수 선택 : 목표변수와 관련된 설명(독립 변수)들을 선택한다.
		- 의사결정나무 형성 : 자료의 구조에 따라 분리 기준과 정지 규칙을 설정하면, 정지규칙이 만족할때까지 나무가 성장한다.
		- 가지치기 : 불필요한 가지를 제거하여 오류와 과적합을 줄인다.
			- 가지치기의 정의
				- 불필요한 가지를 제거하는 것이다.
				- 검증용 데이터를 활용해 예측 정확도를 산출하고 이를 기반으로 불필요한 가지를 제거하거나,
				- 구축된 모형에서 타당성이 없는 규칙을 제거하는 식으로 가지치기를 진행한다.
			- 가지치기의 분리 기준
				- 정확한 예측을 위해서 속하는 자료의 순수도는 증가하고 불순도는 감소하는 방향으로 분류를 진행한다.
		- 타당성 평가 : 형성된 의사결정 트리를 평가하는 단계다. 검증용 데이터를 이용해 모델의 예측 정확도를 평가하거나 이익도표 등의 평가지표를 이용해 의사결정 트리를 평가한다.
		- 해설 및 예측 : 구축된 의사결정 트리를 예측에 적용하고 이를 해석한다.
	- 정지규칙
		- 더이상 트리의 분리가 일어나지 않게하는 규칙이다.
		- 트리의 깊이를 제한하거나, 마디에 속하는 자료가 일정 수 이하일 경우 분할을 정지하는 등의 통제를 한다
- 알고리즘
	- CART : 일반적인 의사결정나무 알고리즘이며, 분순도 측도를 위해 지니계수나 이진분리를 활용한다.
	- C4.5 / C5.0 : 범주형, 이산형 목표변수에만 활요되며, 불순도 측도로 엔트로피 지수를 활용한다. 
	- CHAID : 범주형/이산형 목표변수와 연속형 목표변수에 활용되며 불순도 측도로 카이제곰 통계량을 활용한다.
	- 랜덤포레스트 : 부트스트래핑 기반 샘플링을 활용한 의사결정 나무 생성 이후 배깅 기반 나무들을 모아 [[앙상블 분석]]하여 숲을 형성하게 되는 알고리즘을 말한다.

###### 인공신경망 분석
- 개념
	- 실제 생물의 신경계를 모방해 예측 및 분류를 하는 머신러닝 알고리즘이다.
- 알고리즘
	- 사용되는 비선형함수를 '활성함수'라고 한다.
	- 대표적인 활성함수로는 시그모이드 함수, 소프트맥스 함수, ReLU 함수 등이 있다.
- 계층구조
	- 입력층, 은닉층, 출력층의 세가지로 구성된다. 
- 종류
	- 단층 퍼셉트론
		- 단층 신경망이라고도 하며 입력층이 은닉층을 거치지 않고 바로 출력층으로 연결된다.
	- 다층 퍼셉트론
		- 둘 이상의 퍼셉트론의 중첩으로 입력층, 출력층 사이에 하나 이상의 은닉층을 두어 비선형적인 데이터도 학습할수 있게 한 알고리즘이다. 
###### [[서포트 벡터 머신(SVM)]]
- 개념
	- 분류와 회귀분석에 사용되는 지도학습 알고리즘
	- 서포트 벡터 머신에서는 데이터가 n차원으로 주어졌을때 이러한 데이터를 n-1차원의 초평면으로 분리한다.
	- 결정 경계와 가장 가까운 데이터를 '서포트 벡터'라고 하며, 서포트 벡터와 결정 경계 사이를 '마진'이라고 한다.
	- 데이터가 어느 카테고리에 속할지 판단하는 이진 선형 분류 모델을 만드는 기법으로 분류 모델은 데이터가 사상된 공간에서 경계로 표현되는데 그 중 가장 큰 폭을 가지는 경계를 찾는 알고리즘이다.
	- ![[서포트 벡터의 구성요소.png]]
- 소프트 마진
	- 하드 마진의 단점을 극복하기 위해 최대의 마진을 가지는 동시에 여유변수의 합을 최소로 하는 초평면을 찾는 것
- 커널 기법
	- 현실세계에서 선형으로 완벽히 구분되는 데이터는 드물다.
	- 선형으로 분리되지 않는 데이터를 저차원에서 고차원으로 매핑하여 해결할 수 있다.
###### [[연관분석]]
- 개념
	- 상품이나 서비스를 구매하는 등의 일련의 거래 데이터 안에 존재하는 일정한 연관 규칙을 발견하는 과정이다.
	- 연관 규칙이란 '만약 A항목을 구매했다면 B항목 또한 구매할 것이다' 라는 If A then B의 형태로 표현되는 패턴을 말한다.
	- 주로 거래내역 데이터의 구매항목간에 존재하는 연관성에 대한 규칙을 추론하므로 장바구니 분석이라고도 한다.
- 측정 지표
	- 지지도 : 데이터 전체에서 해당 물건을 고객이 구입하는 확률
	- 신뢰도 : 어떤 데이터를 구매했을 때 다른 제품이 구매될 조건부 확률
	- 향상도 : 두 물건의 구입 여부가 독립인지 확인하는 개념으로 1이면 상호 독립적인 간계, 1보다 크면 양의 상관관계, 1보다 작으면 음의 상관관계다.
- 알고리즘
	- apriori 알고리즘
		- 지지도를 사용해 빈발 아이템 집합을 판별하고 계산의 복잡도를 감소시키는 알고리즘
		- '품목 A의 지지도가 최소지지도보다 작다면 품목 A를 포함하는 모든 아이템 집합의 지지도는 최소지지도보다 작을 것이다.' 라는 아이디어를 바탕으로 부분집합 수를 줄임으로써 계산의 복잡도를 감소시킨다.
	- FP-Growth 알고리즘
		- FP-Tree라는 구조를 통해 최소지지도를 만족하는 빈발 아이템 집합을 추출할 수 있는 알고리즘이다. 
###### [[군집분석]]
- 개념
	- 관측치들의 유사성을 기초해 전체 데이터를 몇 개의 집단으로 나누는 분석 기법으로, 개체를 분류하기 위한 명확한 기준이 존재하지 않는 경우 사용하는 비지도학습의 한 방법
	- 군집분석에는 계층적 군집, 밀도기반 군집, 격자기반 군집, 모형기반 군집, 코호넨맵 등이 있다.
	- 관측치의 유사성을 측정하기 위한 방법으로는 거리측도, 유사성 측도가 있다.
		- 대표적인 거리측도로는 유클리드 거리, 맨해튼 거리 등이 있고,
		- 유사성 측도로는 코사인 거리와 상관계수가 있다.
		- 거리가 가까울 수록 유사성이 크다.
- 거리 측도
	- 변수가 연속형인 경우
		- 유클리드 거리, 맨해튼 거리, 민코프스키 거리, 표준화 거리, 마할라노비스 거리 등
	- 변수가 범주형인 경우
		- 단순 일치 거리, 자카드 거리, 해밍 거리 등
- 계층적 군집
	- 개념
		- 개별 관측치 간의 거리를 계산해서 가장 가까운 관측치부터 결합해감으로써 계층적 트리 구조를 형성하고 이를 통해 군집화를 수행하는 방법
	- 군집 간 거리
		- 군집분석에는 벡터간의 거리 뿐만 아니라, 군집간의 거리에 대한 정의도 필요하다.
		- 여러 연결법을 통해 군집을 생성해보고 유의미한 군집을 형성하는 방법을 적용해야한다.
		- 측정방법
			- 단일연결법, 완전연결법, 평균연결법, 중심연결법, 와드연결법이 있다.
- 비계층적 군집
	- 개념
		- 계층적으로 군집을 형성하지 않고 구하고자하는 군집 수를 사전에 정의해 정해진 군집의 수만큼 형성하는 방법이다.
		- 많은 양의 데이터를 바르게 분류할 수 있으며, 계층적 군집을 선행하여 대략적인 군집 파악 후에 초기 군집 수를 설정한다.
	- 종류
		- k-means 군집
			- 군집 수(k개)를 사전에 정한 뒤 집단 내 동질성과 집단간의 이질성을 높게 전체 데이터를 k개 군집으로 분할하는 알고리즘
		- DBSCAN
			- 밀도 기반 군집분석의 방법으로 개체 간의 거리에 기반을 둔 다른 군집 방법 알고리즘과 다르게 개체들이 밀집한 정도에 기초해 군집을 형성한다.
			- 노이즈가 포함된 데이터셋에 대해서도 효과적으로 군집을 형성할 수 있고, 초기 군집의 수를 설정할 필요가 없다.
		- 가우시안 혼합 모델
			- 데이터가 k개의 정규분포로부터 생성됐다고 가정하는 모델로, 데이터로부터 모수와 가중치를 추정하는 대표적인 모수적 군집 방법이다.
		- SOM(Self Organizing Maps)
			- 코호넨 맵이라고도 불리며, 인공신경망을 기반으로 차원축소와 군집화를 동시에 수행할 수 있는 알고리즘이다. 
			- SOM에서 입력층의 각 뉴런은 경쟁층의 각 뉴런과 유클리드 거리를 통해 거리를 계산하고 비교한다. 
##### 고급 분석기법
###### 범주형 자료 분석
- 개념
	- 독립변수와 종속변수가 모두 범주형 데이터이거나 둘중 하나가 범주형 데이터일 때 사용하는 분석 방법이다.
- 상대적 위험도
	- 상대적 위험도(Relative Risk, RR)는 코흐트 연구에서 주로 사용하는 방법으로 위험인자에 노출됐을 때 질병이 발생할 확률과 위험인자에 노출되지 않았을때 질병이 발생할 확률의 비로 표현된다.
	- 상대적 위험도는 두 확률의 비율로 나타내고, 오즈비는 두 오즈의 비율로 나타낸다는 것에 차이점이 있다.
- 분석기법
	- 카이제곱 검정
		- 범주형 자료 간의 차이를 보여주는 분석 방법이다. 
		- 카이제곱 검정에서는 기대빈도가 5 이하인 셀이 전체의 20%가 넘지 않아야 하며, 이를 어겼을 경우 범주를 그룹화 하여 빈도수가 낮은 셀을 병합해야 한다.
		- 카이제곱 검정은 크게 적합성 검정, 동질성 검정, 독립성 검정 세가지 형태로 나뉜다.
	- t-test
		- 개념
			- 두 집단의 평균을 비교하는 검정 방법으로 독립변수가 범주형이고 종속변수가 수치형일ㄷ 때 사용가능한 범주형 자료 분석 방법이다.
			- 독립변수가 2개의 범주로 구성되어 있는 경우에는 t-test를 사용하고, 3개 이상의 범주로 구성되어 있는 경우에는 분산분석을 사용한다.
		- 구분
			- 단일표본 t-test
				- 한 집단의 평균이 모집단의 평균과 같은지 검증하는 것
				- 예를 들어 한국의 남자 고등학생의 평균 신장이 170cm라고 할때 A 고등학교 남학생들의 평균 신장을 비교한다면 단일표본 t-test를 적용할 수 있다.
			- 대응표본 t-test
				- 동일한 집단의 처치 전후 차이를 알아보기 위해 사용하는 검정 방법
				- 예를 들어 A 집단이 수학 과외를 받았을 때 수학 성적에 차이가 있는지를 알고 싶을 때
			- 독립표본 t-test
				- 서로 다른 모집단에서 추출된 경우 사용할 수 있는 분석방법으로 독립된 두 집단의 평균 차이를 검정한다.
				- 예를 들어 A 그룹과 B 그룹의 평균 수학성적에 차이가 있는지를 분석하고자 할 때
###### 다변량 분석
- 개념
	- 두 개 이상의 변수들을 동시에 분석하는 분석방법을 가리킨다.
	- 각 변수를 개별적으로 분석하지 않고 동시에 분석하며 여러 변수 간의 관계성을 고려한다.
- 분석기법
	- 다변량분산분석
		- 분산분석(ANOVA)의 확장된 형태로 종속변수가 하나고 범주가 2개 이상일 때 각 범주 간 평균을 비교하는 것이라면,
		- 다변량분산분석(MANOVA)는 2개 이상의 종속변수가 주어졌을때 각 범주 간의 평균벡터의 차이를 비교하는 분석 방법이다.
	- 요인분석
		- 변수 간에 존재하는 상호연관성을 바탕으로 데이터를 적은 수의 요인으로 압축 및 요약해 그룹화하는 방법이다.
	- 판별분석
		- 두 개 이상의 모집단으로부터 추출된 표본들을 분석해 각 표본이 어느 모잡단에서 추출된 것인지를 예측하는 분석방법
	- 다차원 척도법

###### 시계열 분석
- 개념
	- 시계열 데이터를 통해 시간의 흐름에 따른 종속변수의 변화를 예측하는 것을 시계열 분석이라고 한다.
	- 주가, 환율, 월별 재고량, 일일 확진자 수 등이 시계열 자료에 해당한다.
- 구성요소
	- 추세요인
		- 장기간 일정한 방향으로 상승 또는 하락하는 경향을 보이는 요인
		- 인구의 증가, 기술의 변화 등과 같은 요인
	- 순환요인
		- 주기가 일정치 않는 변동을 순환요인이라고 하며, 반복 운동하는 형태로 나타난다.
	- 계절요인
		- 일정 주기를 가지고 상하 반복의 규칙적인 변동을 계절 변동이라 한다. 
		- 요일마다 반복되는 주기, 월별, 분기별, 계절별 등 고정적인 주기에 따라 자료의 변동이 반복되는 모든 경우
	- 불규칙요인
		- 위 세가지 요인으로 설명하지 못하는 요인
		- 천재지병, 질병 등과 같은 요인에 의해 발생하는 모든 변동
- 분석기법
	- 이동평균법, 지수평활법, 가법모형, 승법모형 가 있다.
- 시계열모형
	- 자기회귀모형(AutoRegressive, AR모형)
		- 변수들의 자기상관성을 기반으로 한 시계열 모형으로 현시점의 자료를 p 시점 전의 과거 자료를 통해 설명할 수 있는 모델이다.
	- 이동평균모형(Moving Average, MA모형)
		- 이동 평균 과증올 현재 데이터가 과거 백색잡음의 선형 가중합으로 구성된다는 모형
	- 자기회귀누적이동평균모형(ARIMA)
###### 베이지안 기법
- 베이즈 이론
	- 개념
		- 통계학의 확률은 크게 빈도 확률과 베이지안 확률로 구분할 수 있다.
		- 빈도 확률은 객관적으로 확률을 해석하고, 베이지안 확률은 주관적으로 확률을 해석한다. 
	- 구분
		- 빈도 확률
			- 사건이 발생한 횟수의 장기적인 비율을 의미
			- 동전의 숫자 면이 나올 확률이 50%라는 것은 동전을 수백번 던졌을 때 그중 절반은 앞면이 나오고, 나머지 절반은 인물 면이 나오나고 해석한다.
		- 베이지안 확률
			- 어떤 표본을 선택했을 때 해당 표본이 사건 A에 포함된다는 주장의 신뢰도라 할 수 있다.
			- 동전의 숫자 면이 나올 확률이 50%라면 베이지안 확률에서는 동전의 숫자 면이 나왔다는 주장의 신뢰도를 50%라고 해석한다.
			- 지진이 발생할 확률, 내일 아침에 비가 올 확률 등과 같이 빈도를 측정하기 어려운 문제를 측정하고자 하는 사건과 몇가지 확률을 통하여 추정하는 것이다.
		- 베이즈 정리
			- 관측된 데이터의 빈도만 분석하는 것이 아니라 분석자의 사전지식까지 포함해 분석하는 방법이다.
- 나이브 베이즈 분류
	- 개념
		- 나이브 베이즈 분류 모델은 베이즈 정리를 기반으로 한 지도학습 모델로,
		- 스팸 메일 필터링, 텍스트 분류 등에 사용할 수 있다.
		- 예를 들어 신문기사 제목을 통해 카테고리를 분류하는 것을 나이브 베이즈 분류를 통해 수행할 수 있다.
###### 딥러닝 분석
- 개념
	- 인공신경망에 기반하여 연속된 여러 개의 층을 가진 인공신경망을 통해 계측적으로 데이터를 학습하는 방법이다.
- 알고리즘
	- 합성곱신경망(Convolutional Neural Network, CNN)
		- 고양이의 시각피질 연구에 기초한 모델로 이미지를 작은 조각으로 쪼개서 인식한 후 그 정보를 합쳐 하나의 사물로 판단하는 구조를 모방하는 알고리즘이다.
		- 이미지 처리 분야에 자주 사용된다.
	- 순환신경망(Recurrent Neural Network, RNN)
		- 문장이나 시계열 데이터와 같이 순차적인 형태의 시퀀스 데이터 학습에 최적화된 알고리즘
		- 음성, 문장, 시계열 데이터 등 순차적인데이터를 다룰 수 있다.
	- 그밖에도 LSTM, 오토인코더, GAN 등이 있다.
###### 비정형 데이터 분석
- [[텍스트 마이닝]]
	- 개념
		- 텍스트마이닝은 비정형 텍스트에서 특정 단어의 출현 빈도, 단어 간의 연관성, 단어의 긍정/부정의 방향성 등을 파악하고, 이를 통해 의미 있는 정보를 추출하는 방법이다.
		- SNS 데이터를 통해 선거여론을 조사해 선거전략에 활용, 혹은 선거결과를 예측하는 문제 등에 활용 될 수 있다.
	- 주요기능
		- 문서 분류 : 주어진 문서의 내용을 분석해 사전에 정의된 범주에 각 문서를 할당하는 기법
		- 문서 군집 : 상호 관련성이 높은 문서끼리 군집시키는 방법
		- 특징 추출 : 키워드, 날짜, 지명 등의 정보를 추출하는 방법
		- 문서 요약 : 대용량 문서의 복잡도와 길이를 줄이고 요약하는 방법
	- 수행단계
		- 데이터 수집 : 인터넷 상에 기사, 논문, SNS 등 분석에 사용할 텍스트 데이터를 수집하는 단계
		- 텍스트 전처리 : 수집한 코퍼스(말뭉치) 데이터를 용도에 맞게 처리하는 단계다.
			- 클렌징, 토큰화, 불용어 제거, 어간 추출, 표제어 추출 둥의 방법이 있다.
		- 피처 벡터화 : 전처리된 데이터에서 문서별 단어의 사용빈도를 이용해 단어 문서 행렬을 생성하는 단계이다.
		- 텍스트 분석 및 시각화 : 피처 벡터화를 통해 숫자형으로 변환할 데이터를 이용해 분석 및 시각화를 하는 단계다.
		- 오피니언 마이닝 : 감성분석이라고도 한다. 텍스트 내 문장 구조 및 단어를 분석해 긍정/부정 여부를 구분하는 등의 분석을 말한다.
- 사회연결망 분석
	- 개념
		- 사회를 관계성을 중심으로 설명하는 것으로 개인, 집단, 사회의 관계를 네트워크로 파악하는 개념이다.
		- 각 노드의 영향력, 관심사, 성향 등을 분석할 수 있다.
		- 네트워크 구조를 파악하기 위하 기법에는 중심성, 밀도, 중심화 등이 있다.
###### [[앙상블 분석]]
- 개념
	- 주어진 데이터를 여러 개의 학습용 데이터셋으로 분할하고 각각의 학습용 데이터셋을 통해 ==여러 개의 예측모형을 만든 후 여러 예측모형의 결과를 종합해 하나의 최종 결과를 도출하는 방법==
	- 예측모형에서 독립적으로 산출된 결과를 종합하여 예측의 정확도를 향상시킬수 있다.
	- 종속변수의 형태에 따라 회귀분석과 분류분석에 모두 적용할 수 있다.
- 배깅(bagging)
	- bootstrap aggregating의 줄임말
	- 데이터셋에서 중복을 허용하여 랜덤하게 데이터를 추출하는 [[부트스트랩(Bootstrap)|부트스트랩]] 방식을 활용하여 여러 개의 크기가 같은 표본을 추출하고, 각 표본에 대하여 예측모델을 적용한 후 그 결과를 집계하는 방법
	- ![[앙상블 분석_배깅.png]]
- 부스팅
	- 예측력이 약한 모형을 연결하여 순차적으로 학습함으로서 예측력 강한 모형을 만드는 기법
	- 배깅과 차이점은 오분류된 데이터에 가중치를 주어 표본을 추출한다는 점이다.
	- 배깅보다 정확도는 높지만, 과적합 가능성이 있고 이상치에 취약하다는 단점이 있다.
	- ![[앙상블 분석_부스팅 학습 과정.png]]
- [[랜덤 포레스트]]
	- 배깅의 일종으로 배깅에 변수 랜덤 선택 과정을 추가한 것이다.
	- 랜덤 포레스트는 의사결정 트리 기반의 알고리즘으로 여러개의 의사결정 트리가 모여 랜덤 포레스트를 이루는 구조다.
	- 랜덤 포레스트의 가장 큰 특징은 변수를 랜덤하게 선택하여 각 의사결정 트리를 학습시킨다는 것이다. 
	- [[부트스트랩(Bootstrap)|부트스트랩]] 방식을 통해 변수를 선택하므로 입력변수가 아주 많은 경우에도 변수를 제거하지 않고 분석하는 것이 가능하다.![[앙상블 분석_랜덤포레스트 과정.png]]
###### 비모수 통계
- 개념
	- 비모수적 방법은 모집단의 분포를 가정하지 않고 검정을 실시하는 방법
	- 데이터 형태는 명목척도 혹은 서열척도이다.
- 방법
	- 부호 검정 : 중앙값을 통해 가설을 검증하는 방법. 중앙값보다 큰 값은 '+', 작은 값은 '-'를 부여하여 이항분포를 따르게하고 검정을 수행한다.
	- 만-위트니 검정
	- 크루스칼-왈리스 검정
	- 런 검정
***
### 빅데이터 결과 해석

#### 분석 모형 평가 및 개선

##### 분석 모형 평가
###### 분석 평가지표
- 개요
	- 분류분석 모형을 만들었다면 이를 검증하는 과정이 반드시 필요하다.
	- 그래야 그 분석 모형에 실제 데이터를 입력했을 때 원하는 분류분석 결과물을 얻을 수 있을지 알 수 있다.
- [[분석모델 성능 평가|분류분석 평가지표]]
	- 개요
		- 모형이 내놓은 답과 실제 정답이 어느 정도 일치하는지를 판단하는 것이다.
		- 예를들어 스팸 메일인지 아닌지, 환자가 암에 걸렸는지 안 걸렸는지, 은행 대출 승인이 가능한지 불가능한지 등이 있다.
	- 평가지표
		- [[혼동행렬(Confusion Matrix)]]
			- 개념
				- 이진 분류 모형이 예측한 값과 실제 값의 조합을 교차표 형태로 정리한 행렬을 혼동행렬(Confusion Matrix)라고 한다.
				- ![[데이터분석 평가지표_혼동행렬_개념.png]]
			- 평가지표
				- 정확도(Accuracy) : 전체 데이터에서 올바르게 분류한 데이터의 비율
				- 정밀도(Precision) : Positive로 에측한 것중에서 실제 값이 Positive인 비율
				- 재현율(Recall),민감도(Sensitivity), 참긍정율(TPR, True Positive Rate) : 실제 Positive인 값 중 Positive로 분류한 비율
				- 특이도(Specificity), 참부정율(TNR, True Negative Rate) : 실제 Negative인 값 중 Negative로 분류한 비율
				- 거짓긍정률(FPR, False Positive Rate) : 실제 Negative인 값 중 Positive로 잘못 분류한 비율(1-Specificity)
				- F1-스코어 : 정밀도와 재현율의 조화평균으로, 정밀도와 재현율 중 한쪽만 클 때보다 두 값이 골고루 클때 큰 값이 된다. 
				- ![[데이터분석 평가지표_혼동행렬_평가지표.png]]
		- ROC Curve
			- 개념
				- ROC 곡선은 임곗값을 다양하게 조절해 분류 모형의 성능을 비교할 수 있는 그래프로, trade-off 관계인 민감도와 특이도를 기반으로 시각화 한 것이다.
				- ROC 곡선 아래 면적을 AUC(Area Under Curve)라고 하며, AUC가 0.5일 때 분류 능력이 없다고 평가할 수 있고, 면적이 넓을수록, 즉 1에 가까울수록 분류를 잘하는 모형이라 할 수 있다.
				- ![[데이터분석 평가지표__ROC 곡선_AUC개념.png]]
- [[회귀분석 평가지표]]
	- 개요
		- 예측값과 실제값의 차이를 기반으로 한 지표들을 이용해 회귀 모형의 성능을 평가할 수 있다.
		- 예를 들어 미래의 주식 가격 예측, TV 판매량 예측, 비디오 게임 매출액 예측 등이 있다.
	- 평가지표
		- 평균절대오차(MAE)
			- 모형의 예측값과 실제값의 차이를 평균한 값으로 정의하며, 음수오차와 양수오차가 서로 상쇄되는 것을 막기 위해 절댓값을 사용한다.
		- 평균제곱오차(MSE)
			- 모형의 예측값과 실제값 차이를 제굽하여 평균한 값으로 정의한다.
		- 평균제곱근오차(RMSE)
			- 평균제곱근오차(MSE)에 루트를 씌운 값이다. 회귀모형의 평가지표로 실무에서도 자주 사용된다.
		- 평균절대백분율오차(MAPE)
			- 실제값 대비 오차를 평균한 값으로 평균절대오차(MAE)와 같이 절댓값을 모두 더하기 때문에 실제보다 낮은 값으로 예측되는지 높은 값으로 예측되는지 알 수 없다. 
		- 결정계수 R2
			- 주어진 데이터에 회귀선이 얼마나 잘 맞는지, 적합 정도를 평가하는 척도이자 독립변수들이 종속변수를 얼마나 잘 설명하는지 보여주는 지표다.
		- 수정된 결정계수
			- 독립변수의 수가 늘어날 수록 결정계수의 값이 커지는데 이를 개선한 지표로 변수의 수만큼 페널티를 줘서 결정게수보다 작은 값을 가진다.
###### 분석 모형 진단
- [[군집분석]] 모형 진단
	- 개요
		- 군집분석을 수행할 때 수립한 군집분석 모형이 얼마나 타당성을 갖는지 알아보기 위해 군집 간 분산이 최대가 되는 군집개수가 몇개인지, 또 군집 내 분산이 최소가 되는 군집의 개수가 몇 개인지 확인한다.
	- 외부 평가
		- 자카드계수 평가 : 두 데이터 군집간의 유사도를 계산
		- 분류모형 평가 방법을 응용
			- 혼동행렬(confusion matrix)
			- ROC Curve
				- 군집분석 평가에 분류평가 방법을 사용
	- 내부 평가
		- 단순 계산법
			- 전체 데이터의 개수가 n개인 경우, 군집 개수인 k값을 계산
		- 군집 간의 거리를 계산하여 평가
			- 유클리드 거리, 맨해튼 거리, 민코프스키 거리, 표준화 거리 등을 사용
		- 엘보 메소드
			- k-means 분석 시각화
- 회귀분석 모형 진단
	- 개요
		- 선형 회귀 모형이 데이터를 잘 적합하고 있는지를 확인하는 것을 회귀 진단이라고 하며 잔차를 시각화하여 분석을 수행한다.
	- 진단방법
		- 산점도 그리기 및 회귀 모형 구축
		- 회귀모형 해석
		- 잔차진단
		- 선형회귀분석의 가정 만족 여부
- 분석 모형의 오류
	- 분석 모형을 구축할때 흔히 과적합 또는 과대적합 오류에 빠지기 쉽다.
	- 분석모형의 과적합 오류를 해결하기 위해 [[정규화 (Normalization)|정규화]]를 수행하는데, L1정규화(라쏘)와 L2정규화(릿지)가 그것이다.
	- 더 근본적으로 과적합 및 과소적합 오류를 방지하기 위해 교차검증을 진행한다.
###### 교차검증
- 개념
	- 데이터를 나누고 학습하는 과정을 여러차례 반복함으로써 일반화 성능을 평가한다.
- 검증방법
	- k-폴드 교차검증
		- 데이터를 k개의 폴드(fold)라는 파티션으로 나눈다. k는 보통 5~10를 사용한다.
		- 마지막 폴드 검증 셋으로 사용할 때까지 반복하며 측정된 각각의 정확도를 평균낸 값을 모형의 정확도 지표로 평가한다.
	- 홀드아웃
		- 데이터를 랜덤으로 추출해 학습데이터와 테스트 데이터를 분할한다. 
		- 일반적으로 비율은 80:20이며, 검증데이터는 학습데이터에서 분할한다.
	- 리브-p-아웃 교차검증
	- 리브-원-아웃 교차검증
###### 모수 유의성 검정
- 개요
	- 모집단의 검정통계량과 분포를 도출해 검정을 수행하는 모수적 검정과
	- 모집단의 분포에 대해 어떤 가정도 하지 않는 비모수적 검정으로 구분할 수 있다.
- 검정방법
	- 모집단의 평균에 대한 유의성 검정 : Z-검정, T-검정, 분산분석(ANOVA)
	- 모집단의 분석에 대한 유의성 검정 : 카이제곱검정, F-검정
###### 적합도 검정
- 개요
	- 실험에서 얻은 결과가 이론분포와 일치하는 정도를 의미한다.
- 검정방법
	- 카이제곱검정
		- 범주형 데이터를 대상으로 관측된 값들의 빈도수와 기대 빈도수가 의미있가 다른지를 비교한다.
	- 샤피로 윌크 검정
	- 콜모고로프 스미노프 검정
	- Q-Q플롯
##### 분석 모형 개선
###### 과적합 방지
- 학습 데이터 확보
- 교차검증
	- 데이터를 나누고 학습하는 과정을 여러 차례 반복하는 교차검증은 검증용 데이터셋이 고정되어 있지 않아 일반화 성능을 높일 수 있다. 
- 피처의 수를 줄인다.
- 정규화
	- 손실함수에 패널티를 부과해 각각의 영향렬을 줄이고 모형을 단순화해 일반화 성능을 높일 수 있다.
###### 매개변수 최적화
- 경사하강법
	- 함수 값이 급격히 감소하는 방향으로 매개변수 값을 조정하는 것을 반복하여 전역 최솟값을 찾아 나가는 것
	- 해당 알고리즘을 개선하기 위해 여러 알고리즘들이 추가로 개발 되었다.
	- 확률적 경사하강법, 미니 배치 확률척 경사하강법, 모멘텀, AdaGrad, Adam 등이 있다.
- 초매개변수 최적화
	- 초매개변수란 사람이 직접 설정해줘야하는 매개변수를 말한다.
	- 뉴런의 수, 배치크기, 학습률, 은닉층 개수 등이 있다.
		- 학습률 : 기울기 방향으로 얼마나 빠르게 이동할지를 결정한다.
		- 미니배치 크기 : 전체 훈련 데이터셋을 신경망에 넣게 되면 리소스가 비효율적으로 사용되고 시간이 오래 걸리므로 배치 개념을 사용하게 된다.
		- 훈련 반복 횟수 : 전체 훈련 데이터셋이 신경망을 통고한 횟수로 1 epoch는 1회 학습을 통과했다는 뜻이며, 조기 종료를 결정하는 변수이다.
		- 이터레이션 : 하나의 미니배치를 학습할 때 1 Iteration으로 1회 매개변수(파라미터) 업데이트가 진행된다. 
		- 은닉층 개수 : 은닉층 수가 많아질수록 특정 훈련데이터에 더 최적화시킬 수 있다.
###### 분석 모형 융합
- [[앙상블 분석|앙상블 기법]]
	- 개념
		 - 주어진 데이터에서 여러개의 분석 모형들을 만들고 각 학습 결과를 결합해 모형을 구축하는 기법
	- 종류
		- 보팅
			- 서로 다른 알고리즘을 사용한 여러 분석 모형의 결과를 두고 투표를 통해 최종예측 결과를 결정한다.
			- 보팅 방식에는 하드 보팅과 소프트 보팅이 있다.
			- ![[앙상블 분석_보팅.png]]
			- 
		- 배깅
			- 분석모형의 수만큼 [[부트스트랩(Bootstrap)|부트스트랩]] 데이터를 만들고 같은 알고리즘으로 병렬적으로 학습한 결과를 결합해 최종 예측모형을 만든다.
		- 랜덤포레스트
			- 독립변수의 차원을 랜덤하게 감소시킨 다음 그 중에서 독립변수를 선택하는 알고리즘
		- 부스팅
			- 여러개의 연결된 약한 분석 모형을 순차적으로 학습하여 맞추지 못한 부분에 가중치를 부여함으로써 하나의 가한 분석모형을 만드는 앙상블 기법
###### 최종 모형 선정
- 모형 선정이란?
	- 학습 데이터셋으로 구축한 모형 중에서 하나의 최종 분석 모형을 선택하는 절차
- NCS 기반 '모형 성능 평가 및 최종 모형 선정' 과정
	- 데이터셋 준비
		- 먼저 모형 성능을 평가하기 위한 데이터 셋을 준비한다. 
		- 학습용 60%, 모형 검증용 20%, 평가용 20%로 데이터 셋을 보통 나눈다.
	- 분석 모형을 적용해 결과 예측값 도출
		- 각 목적에 맞는 분석기법을 적용하여 예측값을 도출한다.
	- 혼동행렬 만들기
		- 각 분석기법에 대해 머신러닝 모형을 만들고 예측 범주 값 벡터를 만들었다면, 그 결괏값과 실제 범주 값을 비교하는 혼동 행렬을 만든다.
	- 주요 모형 평가지표를 확인하거나 추가로 계산
		- 주요 모형에 대해 성능 평가지표를 계산한다. 
			- 지도학습모형 성능지표
				- 회귀모형 성능지표 : SSE, 결정계수, MAE, MAPE
				- 분류모형 성능지표 : 특이도, 정밀도, 재현율, 정확도
			- 비지도학습모형 성능지표
				- 군집분석 : 군집타당성지표(군집 간 거리, 군집의 지름, 군집의 분산) 고려
				- 연관분석 : 지지도와 신뢰도가 모두 최소한도보다 높은 것으로 평가
	- 모형 성능 평가 비교 결과에 따라서 최종적인 모형 선정
		- 모형 검증 및 평가를 통해 가장 우수한 모형을 선정한다.
#### 분석 결과 해석 및 활용
##### 분석 결과 해석
###### 분석 모형 해석
- 데이터 해석
	- 예시 기반 설명
		- 예시 기반으로 데이터의 관측치 범위와 차원을 결정하는 방법
	- 임베딩 기법
		- 기계가 이해할 수 있게 숫자 형태인 벡터로 바꾼 결과 또는 일련의 과정을 임베딩이라고 한다.
		- 주성분 분석, t-SNE 등의 방법이 있다.
	- 토폴로지 데이터 분석(TDA: Topological Data Analysis)
		- 보존된 기하학적 특성을 연구하는 분야이다.
		- 패턴을 정의하기 어려운 손 모양의 3D 물체를 TDA로 분석하는 예가 가장 대표적이다.
- 분석 모형 및 결과 해석
	- 시각화를 단순히 결괏값을 그래프나 그림으로 표현하는 것에 국한하지 말고, 인사이트 도출을 위한 '연결다리'라는 관점에서 하는 것을 권장한다.
###### 비지니스 기여도 평가
- 개념
	- 빅데이터 분석을 통해 비지니스 성과가 어느 정도 향상됐는지 살펴보는 관점에서 바라봐서는 안된다.
- 빅데이터 비지니스 기여도 평가
	- NCS 기반 빅데이터의 성과관리
		- 빅데이터 성과 기준
		- 측정방법 기획
		- 평가
		- 피드백
	- KPI와 빅데이터
		- 많은 기업이 빅데이터를 도입하고 있으며, KPI에도 이를 도입하여 활용하고 있다.
		- 빅데이터 기반의 평가지표를 수립하여 새로운 객관적인 평가지표를 수립할 수 있다. 
	- 빅데이터 분석을 활용한 마케팅 기여도 평가
	- IT 부분에서의 비지니스 기여도 평가
		- IT 기여도를 정보 시스템 이용으로 인한 이익에 정보시스템이 기여한 정도로 정의하고 재무/품질/이용/효과 측면에서 전략적, 정성적으로 평가하는 활동으로 바라본다.
		- 지표는 아래와 같이 나눌 수 있다.
			- 투자 지표
			- 시스템 구축에 대한 품질 지표
			- 업무 이용 지표
			- 효과 지표

##### 분석 결과 시각화
###### 시각화 개요
- 이해
	- 데이터 분석가가 보여주고 싶은 게 무엇인지에 따라 데이터는 다양한 각도에서 해석이 가능하며, 어떻게 표현하는지에 따라 결과물의 가치를 판단하게 된다.
- 목적
	- 데이터를 분석해서 한눈에 알기 쉽게 표나 이미지 형태로 정리하는 것
	- 커뮤니케이션을 통해 인사이트를 도출하고 정보를 전달하는 것. 경영진의 커뮤니케이션에 활용
- 시각화 프로세스
	- 구조화 단계 : 데이터의 규칙 및 패턴 탐색해서 사용자에 따른 시나리오를 작성하고 스토리를 구성하는 단계
	- 시각화 단계 : 정의된 요건과 스토리를 기반으로 시각화를 구현하는 단계
	- 시각 표현 단계 : 결과물을 검토하고 구조화 단계에서 정의한 초점을 흐리지 않았는지 검토하여 마무리
###### 시공간 시각화
- 시간 시각화
	- 막대그래프, 산점도, 선그래프, 계단식 그래프, 영역차트 등으로 표현이 가능하다.
- 공간 시각화
	- 등치 지역도, 도트 플롯맵, 버블 플롯맵, 등치선도, 입자흐름도, 카토그램 등이 있다.
###### 분포 시각화
- 데이터의 최대값, 최소값, 전체 분포 등을 데이터가 차지하는 영역을 기준으로 시각화 한 것
- 파이차트, 도넛차트, 트리맵, 누적 막대그래프, 누적 영역 차트 등이 있다.
###### 관계 시각화
- 데이터 변수 사이의 연관성이나 분포, 패턴을 찾는 시각화 방법을 말한다.
- 산점도 행렬, 버블 차트, 히스토그램 등이 있다.
###### 비교 시각화
- 다변량 변수를 갖는 자료를 2차원에 효과적으로 표현하여 데이터 간의 차이점 뿐만 아니라 유사성 관계도 확인하는 시각화 방법
- 플로팅바, 간트차트, 히트맵, 평행좌표계, 스타차트 등이 있다.
###### 인포그래픽
- 인포(Information)+그래픽(Graphic)의 합성어로 다양하고 복잡한 데이터를 한눈에 볼 수 있게 표현한 것을 말한다.
##### 분석 결과 활용 
###### 분석 결과 활용의 이해
- 빅데이터 큐레이션
	- 빅데이터 전략을 제시하고 최적의 빅데이터 구축에서 시작해 분석 및 결과 활용까지 전 과정을 지휘하는 활동을 의미
	- 빅데이터 큐레이터는 예측, 요구사항 발견, 고객에게 맞춤형 서비스를 지원 등 비지니스를 지원하고 발전시키는 역할을 한다.
###### 분석 결과 활용 계획 수립
- 시각화 방안 수립
	- 분석결과 시각화는 BI 컨설팅이나 빅데이터 분석 프로젝트 보고 등에 활용된다.
	- 사용자가 누구인지를 명확히 하는 것이 더 효과적으로 정보를 전달하는 데 도움이 된다.
- 협업 계획 수립(절차)
	- 분석 주제 공유 & 협업 목표설정
	- 협업을 위한 R&R 분리
	- 협업 방법, 도구, 업무 수행 기간 협의
	- 예상 결과 및 결과 보고서 항목 정의
###### 분석 모형 전개
- 분석 결과 전개 방안 수립
	- 빅데이터 분석 결과를 기반으로 활용 가능한 정보를 선별하고 분석하여 데이터 변화를 분석
	- 도출된 정보를 기반으로 업무에 적용 가능한 인사이트를 도출하여 비지니스에 적용할 수 있는 계획을 수립힌다.
- 응용 프로그램 전개 방안 수립
	- 분석 결과를 활용할 수 있는 소프트웨어 아키텍처를 구성하고, 시스템간 데이터가 연계될 수 있는 EAI 아키텍쳐를 설계한다.
	- 응용 프로그램 전개 방안 수립 프로세스
		- 서비스 적용 방안 계획 수립
			- 먼저 조직 내부에서 운영 중인 응용 프로그램 중 분석 결과를 활용해 비지니스에 활용할 수 있는 목록을 조사한다.
		- 응용 프로그램 적용 계획 수립
			- 그리고 내/외부적 환경 분석을 통해 분석 결과를 활용할 수 있는 응용 프로그램을 도출한다.
			- 그다음, 응용 프로그램과 빅데이터 플랫폼 간 연계할 수 있는 서비스 아키텍처 방안과 영향도를 분석하고 계획을 수립한다.
		- 아키텍처 평가 및 검증 & 적용 계획
			- 응용 프로그램 적용을 위한 서비스 아키텍처를 평가하여 문제가 없는지 검증하고 적용을 계획한다.
###### 분석 결과 활용 시나리오 개발
- 이해
	- 분석 > 검증 > 평가 > 시각화 > 분석결과 활용계획 수립 이후 단계로 분석모형 명세서를 바탕으로 한다.
- 시나리오 작성
	- 빅데이터 분석 아키텍처를 활용하여 상세히 명세화 한다.
	- ![[빅데이터 분석 결과 활용 시나리오_양식.png]]
- 스토리보드 작업
	- 개요
		- 분석 결과를 활용하는 각각의 사용자(경영진, 이해관계자 등)에 따라 어떻게 효과를 높일 것인지,
		- 또 어떤 시각화 도구를 사용해야 하는지 등을 고민하고 스토리 보드를 작성해야 한다.
	- 사용자별 맞춤형 데이터 표시 수준
		- 상위 사용자들에게는 상위 수준의 결과물을 보여주며 중요한 핵심을 전달하는 것이 중요
		- 실제 업무를 담당하는 참여자의 경우 더 구체화된 '드릴다운' 데이터를 제공
	- 레이아웃 결정
	- 효과적인 시각화 방법 선택
- 스토리텔링 작업
	- 단순히 결론만 보여주는 것이 아닌 다양한 각도에서 데이터를 분석하고, 실험하여, 인사이트가 무엇인지 탐색하고, 대체 이론을 시험하는 반복적인 활동을 의미한다.
	- 수집된 정보를 어떻게 보여주냐에 관심을 두기보다는 정보를 효과적으로 보여주기 위해서 어떤 이야기로 설명해 줄 것인지를 결정해야 한다.
- 분석 정의서 작성
	- 작성한 시나리오 기획서를 기반으로 분석 정의서를 작성한다.
	- 분석 정의서는 빅데이터 분석의 목적을 명확히 기술해야하며, 탐색적 분석 계획 및 기법 수립을 통해 데이터 연관성 및 이상치와 결측치를 찾을 수 있게 작성한다. 
	- 여러가지 분석 모형이 연계해서 수행될 수 있게 분석 모형 절차와 기법을 정의하고,
	- 데이터 품질 및 분석 모형의 지속적 관리를 위한 모형 평가 및 검증 방안을 상세히 기술한다.
###### 분석 모형 모니터링
- 개요
	- 분석 모형 모니터링이란 문석 모형의 정량적, 정성적 평가 기준을 모형화 한 것으로 성능 평가지표를 정의하고 모니터링하여 성과를 측정하는 일련의 과정을 포함하는 개념이다.
- 분석 모형 프로세스
	- 분석 모형 성과 지표 수립
	- 분석 모형 성과 측정 방법 수립
	- 분석 모형 성과 평가 실행
	- 분석 모형 성과 평가 피드백
- 분석 모형 성능 평가지표 수립
	- 분석 모형 성능 평가지표 수립
		- 빅데이터 분석에 대한 체계, 탐색적 분석 모형의 역량, 적합성을 평가지표로 작성한다.
	- ![[빅데이터 분석 성과 지표 정의서_양식.png]]
- 분석 모형 성능 평가 양식 작성
	- 기본 평가지표를 포함하여 고객의 요구사항 중 우선순위가 높은 항목을 상세화 하여 분석 모형 평가표에 기입한다.
	- ![[빅데이터 분석 모델 결과 평가표_양식.png]]
- 운영 관리 체계 수립
	- 개요
		- 분석 모형의 생명 주기를 설정하고 주기적인 평가 및 모니터링을 실시하여 유지보수, 재구축 방안 
		- 모형 업데이트를 자동화하는 방안 적용 계획을 수립한다.
	- 절차
		- 모형의 연속성 확보를 위해 구현 시스템의 요건 정의서를 조사한다.
		- 분석 프로세스 및 도구를 이용하여 모형 발전 계획을 수립한다.
		- 지속 관리 방안을 수립한다.
###### 피드백과 분석 모형 리모델링
- 성과 평가 보고서 목표와 실적과의 차이 분석
- 문제점 도출 및 개선방안 도출
- 성과관리 보상 체계 수립
- 평가 기준 타당성 검토
- 피드백과 분석 모형 리모델링

***

***
# 출처

***
# 관련 노트
[[PJT_빅데이터 분석 이론 이해하기]]
***
# 외부 링크

