---
Created: 2023-10-08 14:23
tags:
  - AI/머신러닝
aliases:
  - 클러스터링 (Clustering)
---

# 개요
군집분석은 비지도학습의 한 방법으로 전체 데이터를 몇 개의 집단으로 나누는 분석 기법이다. 개체를 분류하기 위한 명확한 기준이 존재하지 않을때 사용하는 분석 방법이다.

# 내용
### 개념
- 관측치들의 유사성을 기초해 전체 데이터를 몇 개의 집단으로 나누는 분석 기법으로, 개체를 분류하기 위한 명확한 기준이 존재하지 않는 경우 사용하는 비지도학습의 한 방법
- 군집분석에는 계층적 군집, 밀도기반 군집, 격자기반 군집, 모형기반 군집, 코호넨맵 등이 있다.
- 관측치의 유사성을 측정하기 위한 방법으로는 거리측도, 유사성 측도가 있다.
	- 대표적인 거리측도로는 유클리드 거리, 맨해튼 거리 등이 있고,
	- 유사성 측도로는 코사인 거리와 상관계수가 있다.
	- 거리가 가까울 수록 유사성이 크다.
### 거리 측도
- 변수가 연속형인 경우
	- 유클리드 거리, 맨해튼 거리, 민코프스키 거리, 표준화 거리, 마할라노비스 거리 등
- 변수가 범주형인 경우
	- 단순 일치 거리, 자카드 거리, 해밍 거리 등

### 군집 방법
#### 계층적 군집
- 개념
	- 개별 관측치 간의 거리를 계산해서 가장 가까운 관측치부터 결합해감으로써 계층적 트리 구조를 형성하고 이를 통해 군집화를 수행하는 방법
- 군집 간 거리
	- 군집분석에는 벡터간의 거리 뿐만 아니라, 군집간의 거리에 대한 정의도 필요하다.
	- 여러 연결법을 통해 군집을 생성해보고 유의미한 군집을 형성하는 방법을 적용해야한다.
	- 측정방법
		- 단일연결법, 완전연결법, 평균연결법, 중심연결법, 와드연결법이 있다.
- 특징
	- 한 번 군집에 할당된 객체는 다시 분리되지 않는 것이 특징
	- 비계층적 방법에 비해 계산량이 많고 시간이 오래 걸림

#### 비계층적 군집
- 개념
	- 계층적으로 군집을 형성하지 않고 구하고자하는 군집 수를 사전에 정의해 정해진 군집의 수만큼 형성하는 방법이다.
	- 많은 양의 데이터를 바르게 분류할 수 있으며, 계층적 군집을 선행하여 대략적인 군집 파악 후에 초기 군집 수를 설정한다.
- 종류
	- k-means 군집
		- 군집 수(k개)를 사전에 정한 뒤 집단 내 동질성과 집단간의 이질성을 높게 전체 데이터를 k개 군집으로 분할하는 알고리즘
	- 밀도 기반 클러스터링(DBSCAN)
		- 밀도 기반 군집분석의 방법으로 개체 간의 거리에 기반을 둔 다른 군집 방법 알고리즘과 다르게 개체들이 밀집한 정도에 기초해 군집을 형성한다.
		- 노이즈가 포함된 데이터셋에 대해서도 효과적으로 군집을 형성할 수 있고, 초기 군집의 수를 설정할 필요가 없다.
	- 확률 분포 기반 클러스터링(가우시안 혼합 모델)
		- 데이터가 k개의 정규분포로부터 생성됐다고 가정하는 모델로, 데이터로부터 모수와 가중치를 추정하는 대표적인 모수적 군집 방법이다.
	- SOM(Self Organizing Maps)
		- 코호넨 맵이라고도 불리며, 인공신경망을 기반으로 차원축소와 군집화를 동시에 수행할 수 있는 알고리즘이다. 
		- SOM에서 입력층의 각 뉴런은 경쟁층의 각 뉴런과 유클리드 거리를 통해 거리를 계산하고 비교한다. 

### 사용 예시
- 고객 분류
	- 고객을 구매 이력이나 웹사이트 내 행동 등을 기반으로 클러스터로 모을 수 있음
	- 동일 군집 내의 고객이 좋아하는 콘텐츠를 추천하는 추천 시스템을 만들 수 있음
- 이상치 탐지
	- 각 군집에 대한 데이터 친화성을 측정할 수 있음
	- 제조 분야의 결함 탐지, 부정 거래 감지 등에서 활용 가능
### 알고리즘
#### K-means
- 정의
	- 비지도 학습 기반의 대표적인 비계층적 군집 알고리즘
	- 서로 유사한 데이터는 동일 그룹으로, 유사하지 않은 데이터는 다른 그룹으로 분류하는 군집분석
	- 대표적인 K-means 알고리즘은 K(클러스터 중심 개수), means(각 클러스터 중심과의 평균 거리)로 구성된다.
	- 데이터셋에서 K개의 centroids를 임시 지정한 뒤 가장 가까운 Centroids가 속한 그룹에 할당, 그리고 다시 centroid를 업데이트하여 반복함으로써 각 클러스터와 거리 차이의 분산을 최소화 하는 방식으로 동작함.
- 형성과정 
	- Step 1 랜덤하게 K개(사전의 정의)의 중앙점을 지정
	  ![[군집분석_K-means 참고이미지1.png]]
	- Step 2 유클리드 거리를 이용하여 군집의 중심점과의 거리 계산 및 가장 거리가 가까운 군집으로 군집 할당
	  ![[군집분석_K-means 참고이미지2.png]]
	- Step 3 군집 내 데이터의 평균을 계산 후 새로운 중심점으로 배정 및 중심점이 더 이상 이동 안 할 때까지 반복
	  ![[군집분석_K-means 참고이미지3.png]]

#### DBSCAN 
- 특징
	- K-means 군집의 경우 군집간의 거리를 이용하기 때문에 선형적인 결정 경계를 가지나,
	- DBSCAN의 경우 밀도 기반 클러스터링으로 점들이 몰려 있어서 밀도가 높은 부분을 군집화하는 방식
- 형성과정
	- ![[군집분석_DBSCAN 설명.png]]
		- P4는 노이즈에 해당됨
### 분석 수행
- 분석 목표
	- iris 데이터셋으로 K-means 클러스터링을 사용하여 비슷한 붓꽃끼리 그룹화하고 성능을 평가해본다.
- 접근 방법
	- 변수들 간의 상관성을 시각화해본다.
	- 라벨이 없다는 가정하에 K-means 알고리즘으로 데이터를 그룹화시킨다.
###### 필요 패키지 임포트
~~~python
## 군집분석을 이용한 문제해결 : K-means 알고리즘

## 1. 필요 패키지 임포트(import)
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.cluster import KMeans        # K-Means 패키지 임포트
~~~

###### 데이터 불러오기
~~~python
## 2. 데이터 불러오기
df = pd.read_csv("http://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv")
~~~

###### 데이터 살펴보기
~~~python
## 3. 데이터 살펴보기
df
~~~

###### 데이터 전처리
~~~python
## 4. 데이터 전처리
# species 컬럼의 값을 0,1,2로 레이블인코딩 한다.
from sklearn.preprocessing import LabelEncoder
df["species"] = LabelEncoder().fit_transform(df["species"])
df.head()

df_copy = df        # 기존 데이터프레임 복사본 생성
~~~

###### 분석 데이터셋 준비
- 데이터 특징 핵심인자 탐색을 위한 사전 시각화로 pairplot() 함수를 이용
	- 데이터 특징들 간의 상관관계를 표현한 결과에서 petal length를 통해 종류 구분이 가능함을 판단할 수 있음
~~~python
## 5. 분석 데이터셋 준비
# 변수간 상관관계 시각화
import seaborn as sns
from matplotlib import pyplot as plt            # 그래프 시각화
from mpl_toolkits.mplot3d import Axes3D
from mpl_toolkits.mplot3d import proj3d
sns.pairplot(df, hue="species")                 # 데이터특징들간의 상관관계 표현
plt.show()
~~~
![[군집분석_변수간 상관관계 시각화.png]]
###### 데이터 분석 수행
- 사이킷런의 K-means 모델인 KMeans 클래스(함수) 사용
	- KMeans 객체를 생성하고 fit() 함수에 붓꽃 데이터프레임을 학습시킨다.
- 기존 데이터에 예측된 군집 결과를 붙여서 출력
	- 결과 표를 보면 sepecies와 cluster가 0과 1로 다르게 나오는데, K-means에서는 값 자체보다 유사항 데이터를 같은 값으로 묶어주고 있는지가 중요하다. 
	- 즉, 결과값이 유사한 붓꽃데이터를 같은 종으로 일관성있게 구분하고 있음을 알 수 있다.
~~~python
## 6. 데이터분석 수행
# KMeans 객체 생성
cluster1 = KMeans(n_clusters=3, n_init=10, max_iter=500, random_state=42, algorithm='auto')

# 생성모델로 데이터 학습
cluster1.fit(df)
KMeans(max_iter=500, n_clusters=3, random_state=42)

# 결과 값을 변수에 저장
cluster_center = cluster1.cluster_centers_     # 각 군집의 중심점 결과 저장 
cluster_prediction = cluster1.predict(df)      # 각 예측군집 결과 저장
print(pd.DataFrame(cluster_center))
print(cluster_prediction)

# 기존 데이터에 예측된 군집 결과를 붙인다.
df_copy["cluster"] = cluster_prediction
df_copy
~~~

~~~
          0         1         2         3         4
0  6.622449  2.983673  5.573469  2.032653  2.000000
1  5.006000  3.428000  1.462000  0.246000  0.000000
2  5.915686  2.764706  4.264706  1.333333  1.019608

[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 2 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0]
~~~

||sepal_length|sepal_width|petal_length|petal_width|species|cluster|
|---|---|---|---|---|---|---|
|0|5.1|3.5|1.4|0.2|0|1|
|1|4.9|3.0|1.4|0.2|0|1|
|2|4.7|3.2|1.3|0.2|0|1|
|3|4.6|3.1|1.5|0.2|0|1|
|4|5.0|3.6|1.4|0.2|0|1|
|...|...|...|...|...|...|...|
|145|6.7|3.0|5.2|2.3|2|0|
|146|6.3|2.5|5.0|1.9|2|0|
|147|6.5|3.0|5.2|2.0|2|0|
|148|6.2|3.4|5.4|2.3|2|0|
|149|5.9|3.0|5.1|1.8|2|0|

150 rows × 6 columns
###### 성능평가 및 시각화
- 비지도학습인 K-means는 실제 정답이 없으므로 일반적인 성능평가 대신에 적절한 K개를 설정하였는지 평가할 수 있다.
- K 개수와 inertia 비교시각화로 K=3일 때가 빠르게 줄어들기 시작한 시점임을 알 수 있다.
~~~python
## 7. 성능평가 및 시각화
# 적절한 K에 대해 붓꽃 데이터프레임을 넣어 K와 inertia를 비교
# 값(3)이 적합한 변화시점임을 알 수 있음
scope = range(1,10)
inertias = []

for k in scope:
    model = KMeans(n_clusters=k)
    model.fit(df)
    inertias.append(model.inertia_)

# K 개수와 Inertia 비교 시각화
plt.figure(figsize=(4,4))

plt.plot(scope, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.show()
~~~

![[군집분석_성능평가_K개수와 inertia 비교.png]]

### 군집모델 평가
- 군집 알고리즘의 경우 정답 값이 존재하는 경우 ARI(Adjusted Rand Index) 또는 NMI(Normalized Mutual Information)과 같은 지표를 이용하여 평가

- 군집의 번호는 랜덤하게 배정되기 때문에 위치의 값을 비교하는 정확도는 성능이 부정확하게 나올 수 있음

- 군집 알고리즘은 비지도 학습에서 사용되는 방법으로 레이블 되지 않은 데이터가 있을 때 많이 이용하는 방법

- 따라서 ARI 또는 NMI와 같은 정답 값을 이용하여 모델의 성능을 계산하기에는 많은 어려움이 있음

- 각 데이터와 가장 가까운 중심점 사이의 평균 제곱 거리를 이용하는 이너샤(inertia)를 사용하면 K값에 따른 이너샤의 변화를 확인할 수 있고 적정 군집 수 K를 정의할 수 있음

- 실제값이 없다면 실루엣 가중치를 사용함
    - 한 클러스터 안에 데이터들이 다른 클러스터와 비교해서 얼마나 비슷한 지를 나타냄
        - 응집도와 분리도를 이용하여 계산
    - 클러스터가 최적화되었다면 실루엣 가중치는 1에 가까운 숫자로 나옴

~~~ python
from sklearn.metrics import accuracy_score
from sklearn.metrics import adjusted_rand_score
from sklearn.metrics import normalized_mutual_info_score

model = KMeans(n_clusters=3)
model.fit(X) 

print('ACC:', accuracy_score(Y, model.labels_))
print('ARI:', adjusted_rand_score(Y, model.labels_))
print('NMI:', normalized_mutual_info_score(Y, model.labels_))
~~~

~~~
ACC: 0.24
ARI: 0.7302382722834697
NMI: 0.7581756800057785
~~~
# 출처


# 관련 노트


# 외부 링크

