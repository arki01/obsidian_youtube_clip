---
Created: 2023-10-08 14:00
tags:
  - AI/머신러닝
aliases:
  - 앙상블 기법
  - 앙상블 학습
---

# 개요
여러 개의 예측모형을 만든 후 결과를 종합해 하나의 최종 결과를 도출하는 방법이다. 
종속변수의 형태에 따라 회귀, 분류 분석에 모두 적용할 수 있어 분석시에 여러모로 잘 사용하게 되는 방식이다.

# 내용

### 개념
- 주어진 데이터를 여러 개의 학습용 데이터셋으로 분할하고 각각의 학습용 데이터셋을 통해 ==여러 개의 예측모형을 만든 후 여러 예측모형의 결과를 종합해 하나의 최종 결과를 도출하는 방법==
- 예측모형에서 독립적으로 산출된 결과를 종합하여 예측의 정확도를 향상시킬수 있다.

### 주요 기법
#### 보팅 (Voting)
- 서로 다른 알고리즘을 사용한 여러 분석 모형의 결과를 두고 투표를 통해 최종예측 결과를 결정한다.
- 보팅 방식에는 하드 보팅과 소프트 보팅이 있다.
![[앙상블 분석_보팅.png]]
##### 하드 보팅 (Hard voting)
- 각 분류기가 예측한 결과를 집계해 가장 많이 나온 결과를 채택하는 방식이고,
##### 소프트 보팅 (Soft voting)
- 각 분류기기 예측한 레이블 값의 결정 확률을 평균 내어 가장 확률이 높은 레이블을 최종 결괏값을 채택한다.

~~~ python
# 환경 설정
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split

# 분류 데이터 가져오기
df = pd.read_csv('/mnt/elice/dataset/breast_cancer.csv')

# 데이터 분할하기
X = df.iloc[:, :-1]
Y = df.iloc[:, -1]

# 학습용과 평가용으로 데이터 분할
 # 층화 분리 적용
x_train, x_test, y_train, y_test = train_test_split(X, Y, stratify=Y, random_state=0)

# Voting에 참여할 분류기 생성
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

knc = KNeighborsClassifier()
lr = LogisticRegression()
dtc = DecisionTreeClassifier()

# hard voting 분류 모델 생성
from sklearn.ensemble import VotingClassifier
hard = VotingClassifier(
    [('knn', knc), ('lr', lr), ('dtc', dtc)],
    voting='hard'
)

# 모델 학습
hard.fit(x_train, y_train) # 학습용 데이터만 사용

# 모델 평가 (정확도)
print('학습 데이터 성능 :', hard.score(x_train, y_train))
print('평가 데이터 성능 :', hard.score(x_test, y_test))

# 모델 예측
p_test = hard.predict(x_test) # test 데이터에 대한 예측 수행

# classification_report 계산
from sklearn.metrics import classification_report
print(classification_report(y_test, p_test))
~~~

~~~ python
# soft voting 분류 모델 생성
from sklearn.ensemble import VotingClassifier
soft = VotingClassifier(
    [('knn', knc), ('lr', lr), ('dtc', dtc)],
    voting='soft'
)

# 모델 평가 (정확도)
print('학습 데이터 성능 :', soft.score(x_train, y_train))
print('평가 데이터 성능 :', soft.score(x_test, y_test))

# 모델 예측
p_test = soft.predict(x_test) # test 데이터에 대한 예측 수행
~~~
####  배깅 (Bagging)
- bootstrap aggregating의 줄임말
- 데이터셋에서 중복을 허용하여 랜덤하게 데이터를 추출하는 [[부트스트랩(Bootstrap)|부트스트랩]] 방식을 활용하여 여러 개의 크기가 같은 표본을 추출하고, 각 표본에 대하여 예측모델을 적용한 후 그 결과를 집계하는 방법
- ![[앙상블 분석_배깅.png]]
- 분류 모델에서는 보팅을 회귀 모델에선 평균으로 집계

##### [[랜덤 포레스트]]
- ==배깅의 일종으로 배깅에 변수 랜덤 선택 과정을 추가한 것이다.==
- 랜덤 포레스트는 의사결정 트리 기반의 알고리즘으로 여러개의 의사결정 트리가 모여 랜덤 포레스트를 이루는 구조다.
- 랜덤 포레스트의 가장 큰 특징은 변수를 랜덤하게 선택하여 각 의사결정 트리를 학습시킨다는 것이다. 
- [[부트스트랩(Bootstrap)|부트스트랩]] 방식을 통해 변수를 선택하므로 입력변수가 아주 많은 경우에도 변수를 제거하지 않고 분석하는 것이 가능하다.
- 각 결정 트리는 비교적 예측을 잘하지만, 데이터 일부에 과대적합되는 경향을 가진다는 점을 이용
- 서로 다른 데이터를 학습하여 과대적합된 트리를 많이 만들고 결과를 집계함으로써 과대적합이 상쇄되어 성능이 향상
- ![[앙상블 분석_랜덤포레스트 과정.png]]

#### 부스팅 (Boosting)
- ==예측력이 약한 모형을 연결하여 순차적으로 학습함으로서 이전 모델에서 발생한 오류에 대해 높은 가중치를 부여하는 방법==
- 배깅보다 정확도는 높지만, 과적합 가능성이 있고 이상치에 취약하다는 단점이 있다.
- ![[앙상블 분석_부스팅 학습 과정.png]]
~~~ python
# GradientBoosting 분류 모델 생성
from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier()

# 모델 학습
gbc.fit(x_train, y_train) # 학습용 데이터만 사용

print('학습 데이터 성능 :', gbc.score(x_train, y_train))
print('평가 데이터 성능 :', gbc.score(x_test, y_test))

# 모델 예측
p_test = rfc.predict(x_test) # test 데이터에 대한 예측 수행

# classification_report 계산
from sklearn.metrics import classification_report
print(classification_report(y_test, p_test))
~~~
# 출처


# 관련 노트


# 외부 링크

