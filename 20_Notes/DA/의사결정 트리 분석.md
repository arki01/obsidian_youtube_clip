---
Created: 2023-10-22 22:12
tags:
  - AI/머신러닝
aliases:
  - 의사결정나무
---

# 개요
데이터를 학습하여 데이터 내에 존재하는 규칙을 찾아내고, 이 규칙을 나무구조로 모형화해 분류와 예측을 수행하는 분석 방법이다.
# 내용
### 개념
- 데이터를 학습하여 데이터 내에 존재하는 규칙을 찾아내고, 이 규칙을 나무구조로 모형화해 분류와 예측을 수행하는 방법이다.
- 올바른 분류를 위해서 상위노드에서 하위노드로 갈수록 집단 내에서는 동질성을 가지고, 집단간에는 이질성이 커져야한다.
- 중간 노드가 많다는 것은 규칙이 복잡하다는 이야기이므로, 모델이 과적합되기 쉽기 때문에 나무의 깊이를 적절하게 조절해야한다
### 종류
- 분류나무 (이산형 목표변수)
	- 정보 균일도(분류된 세트에 포함된 정보가 비슷한 정도)가 높게 하도록 분할
		- 정보균일도를 측정하는 방법은 [[엔트로피]]를 이용하는 방법과 [[지니계수]]를 이용하는 방법이 있음 
	- 이산형 목표변수에 따른 빈도 기반의 분리에 사용한다.
	- 상위노드에서 가지 분할을 진행할 때 카이제곱 통계량의 P-value, 지니지수, 엔트로피지수 등이 분리 기준에 활용된다.
- 희귀나무 (연속형 목표변수)
	- 잔차 제곱합이 최소가 되도록 분할
		- 영역을 분할 수 각 영역의 순도 증가 및 불순도 또는 불확실성이 최대한 감소하도록 학습
	- 연속형 목표변수에 따른 평균/표준편차 기반 분리에 사용하며 특정 의미를 지니는 실수값을 출력한다.
	- 상위노드에서 가지분할을 진행할때 F-통계량의 p-value, 분산의 감소량 등이 분리의 기준이 된다.
### 분석과정
- 의사결정트리 분석 과정
	- 변수 선택 : 목표변수와 관련된 설명(독립 변수)들을 선택한다.
	- 의사결정나무 형성 : 자료의 구조에 따라 분리 기준과 정지 규칙을 설정하면, 정지규칙이 만족할때까지 나무가 성장한다.
	- 가지치기 : 불필요한 가지를 제거하여 오류와 과적합을 줄인다.
		- 가지치기의 정의
			- 불필요한 가지를 제거하는 것이다.
			- 검증용 데이터를 활용해 예측 정확도를 산출하고 이를 기반으로 불필요한 가지를 제거하거나,
			- 구축된 모형에서 타당성이 없는 규칙을 제거하는 식으로 가지치기를 진행한다.
		- 가지치기의 분리 기준
			- 정확한 예측을 위해서 속하는 자료의 순수도는 증가하고 불순도는 감소하는 방향으로 분류를 진행한다.
	- 타당성 평가 : 형성된 의사결정 트리를 평가하는 단계다. 검증용 데이터를 이용해 모델의 예측 정확도를 평가하거나 이익도표 등의 평가지표를 이용해 의사결정 트리를 평가한다.
	- 해설 및 예측 : 구축된 의사결정 트리를 예측에 적용하고 이를 해석한다.
- 정지규칙
	- 더이상 트리의 분리가 일어나지 않게하는 규칙이다.
	- 트리의 깊이를 제한하거나, 마디에 속하는 자료가 일정 수 이하일 경우 분할을 정지하는 등의 통제를 한다
### 알고리즘
- CART : 일반적인 의사결정나무 알고리즘이며, 분순도 측도를 위해 지니계수나 이진분리를 활용한다.
- C4.5 / C5.0 : 범주형, 이산형 목표변수에만 활요되며, 불순도 측도로 엔트로피 지수를 활용한다. 
- CHAID : 범주형/이산형 목표변수와 연속형 목표변수에 활용되며 불순도 측도로 카이제곰 통계량을 활용한다.
- 랜덤포레스트 : 부트스트래핑 기반 샘플링을 활용한 의사결정 나무 생성 이후 배깅 기반 나무들을 모아 [[앙상블 분석]]하여 숲을 형성하게 되는 알고리즘을 말한다.

### 분석 수행

##### 의사결정나무 - 분류(예측)분석 수행
타이타닉 데이터셋에서 탑승자들의 여러 속성 데이터를 기반으로 생존여부(Survived)를 예측한다.
###### 필요 패키지 임포트
~~~ python
## 의사결정나무
import numpy as np
import pandas as pd
import sklearn

# 의사결정나무 분류모델을 위한 패키지 임포트
from sklearn.tree import DecisionTreeClassifier

# 학습 및 테스트 데이터셋 분리를 위한 패키지 임포트
from sklearn.model_selection import train_test_split         
~~~
###### 데이터 불러오기

~~~ python
df = pd.read_csv("http://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")
~~~
###### 데이터 살펴보기

~~~ python
## 데이터 살펴보기
df      # 데이터프레임 전체 출력
df.info()
~~~
###### 데이터 전처리
~~~ python
## 데이터 전처리
# Age 컬럼의 결측값을 평균으로 대치한다.
d_mean = df["Age"].mean()
df["Age"].fillna(d_mean, inplace=True)

# Embarked 컬럼의 결측값을 최빈값으로 대치한다.
d_mode = df["Embarked"].mode()[0]
df["Embarked"].fillna(d_mode, inplace=True)

#  Sex 컬럼의 값을 1과 0으로 레이블인코딩 한다.
from sklearn.preprocessing import LabelEncoder
df["Sex"] = LabelEncoder().fit_transform(df["Sex"])

# Embarked 컬럼의 값을 레이블인코딩한다.
from sklearn.preprocessing import LabelEncoder
df["Embarked"] = LabelEncoder().fit_transform(df["Embarked"])

# 파생변수 생성 - SibSp, Parch 값을 더해서 FamilySize 컬럼을 생성한다.
df["FamilySize"] = df["SibSp"] + df["Parch"]

df.describe()
df
~~~
###### 분석 데이터셋 준비
~~~ python
## 분석 데이터셋 준비
# X는 독립변수(설명변수), y는 종속변수(목표변수)
X = df[["Pclass", "Sex", "Age", "Fare", "Embarked", "FamilySize"]]
y = df["Survived"]

# 분석 데이터셋 분할(8:2)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
~~~

~~~
(712, 6)
(179, 6)
(712,)
(179,)
~~~

###### 데이터 분석 수행
- 주어진 데이터로 탑승자의 생존을 구분하는 분류문제이다.
- 분류를 위한 알고리즘 중에서 의사결정나무를 이용한다.
	- 사이킷런의 DecisionTreeClassifier를 이용한다.
	- DecisionTreeClassifier 객체를 생성하고 fit() 함수에 학습용 데이터(X_train)와 결정값 데이터(y_train)를 입력해 호출하면 학습이 수행된다.
- 학습이 완료된 dt 객체에서 테스트 데이터셋으로 분류(예측) 수행
	- predict() 함수에 테스트 데이터셋 X_test를 입력값으로 준다.
	- X_test에 대해서 분류(예측)가 수행되며, 그 결과를 지정한 변수(pred)에 저장한다.
~~~ python
## 데이터 분석 수행
# 의사결정나무 객체 생성
dt = DecisionTreeClassifier(random_state=11)
dt.fit(X_train, y_train)        # 학습 수행

# 학습이 완료된 dt객체에서 테스트 데이터셋으로 예측 수행
pred = dt.predict(X_test)
~~~
###### 성능평가 및 시각화
- 분류(예측) 결과(pred)와 실제 분류 결과(y_test)를 비교하여 정확도를 평가한다.
- 사이킷런의 accuracy_score() 함수로 정확도 측정
	- 첫번째 파라미터로 분류데이터셋(y_test), 두번째 파라미터로 분석결과 분류(예측)된 데이터셋(pred)를 입력한다.
	- 모델의 정확도는 0.787인 것을 확인할 수 있으며, 79% 정확도로 생존자를 분류(예측)하였다.
~~~ python
## 성능평가 및 시각화
# 모델 성능 - 정확도 측정
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, pred)
print(acc)
~~~

~~~
0.7877094972067039
~~~


##### 의사결정나무 - 회귀분석 수행
- 정의
	- 분류 기능과는 달리 각 항목에서의 범주를 예측하는 것이 아니라 어떠한 값 자체를 예측하는 것
	- 연속형 레이블을 가지는 데이터가 비선형인 경우 Linear Model을 사용시 성능이 좋지 않으므로, 이러한 경우 Regression Tree를 이용할 수 있음
- 분석 목표
	- 1999년 미국 캘리포니아 인구가구 통계 데이터셋에서 주택중위가치(median_house_value)에 영향을 주는 변수들을 찾아보고, 이 변수들을 포함하는 트리모델을 생성하여 성능을 평가해본다.
- 접근 방법
	- 종속변수는 median_house_value로 한다.
	- 상관성이 있다고 판단되는 변수를 선택한 후 각각에 대해 의사결정나무 알고리즘을 적용하여 모델을 생성한다.
	- 평균제곱오차(MSE) 값을 구해서 모델 성능을 평가한다.
###### 필요 패키지 임포트
- 의사결정나무 분석을 위해서 사이킷런의 DecisionTreeRegressor 클래스를 사용한다.
~~~python
## 의사결정나무 알고리즘

## 1. 필요 패키지 임포트
import numpy as np
import pandas as pd
import sklearn
import matplotlib.pyplot as plt     # 맷플롯립 패키지 임포트

# 의사결정 모델을 위한 패키지 임포트
from sklearn.tree import DecisionTreeRegressor
# 학습 및 테스트 데이터셋 분리를 위한 패키지 임포트
from sklearn.model_selection import train_test_split
~~~
###### 데이터 불러오기
~~~python
## 2. 데이터 불러오기
df = pd.read_csv("https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv")
~~~
###### 데이터 탐색하기 
~~~python
## 3. 데이터 살펴보기
df
~~~
###### 데이터 전처리
- 변수들의 상관관계 분석시에 median_income을 제외하면 median_house_value 와의 상관성이 낮은 것을 알 수 있다.
- 전체 컬럼을 독립변수로 사용해 분석을 수행해 본다.
~~~python
## 4. 데이터 전처리
# 결측값이 있는 행 전체 제거
df = df.dropna(axis=0)

# ocean_proximity 는 범주형 값이므로 제거
df = df.drop("ocean_proximity", axis=1)

# 변수들간의 상관관계 분석
corr = df.corr(method = "pearson")
print(corr)
~~~

~~~
                    median_house_value  
longitude                    -0.045398  
latitude                     -0.144638  
housing_median_age            0.106432  
total_rooms                   0.133294  
total_bedrooms                0.049686  
population                   -0.025300  
households                    0.064894  
median_income                 0.688355  
median_house_value            1.000000  
~~~
###### 분석 데이터셋 준비
~~~python
## 5. 분석 데이터셋 준비
# median_house_value를 제외한 나머지를 독립변수로 함
X = df.drop("median_house_value", axis=1)
y = df["median_house_value"]        # 종속변수(목표변수)

# 분석 데이터셋 분할(7:3)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)

print("\n")
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
~~~

~~~
(14303, 8)
(6130, 8)
(14303,)
(6130,)
~~~
###### 데이터 분석 수행
- 사이킷런의 의사결정나무 분석 모듈인 DecisionTreeRegressor을 사용
- 학습이 완료된 dtr 객체에서 데이터셋으로 예측을 수행
~~~python
## 6. 데이터 분석 수행
# DecisionTreeRegressor 객체 생성
dtr = DecisionTreeRegressor(max_depth=3, random_state=43)
dtr.fit(X_train, y_train)     # 학습 수행

# 학습이 완료된 dtr객체에서 테스트 데이터셋으로 예측 수행
pred = dtr.predict(X_test)
~~~
###### 성능평가 및 시각화
- 의사결정나무 분석의 평가는 평균제곱오차(MSE)로 예측 정확도를 판단할 수 있다. MSE 값이 작을수록 모형의 예측 능력이 좋다고 판단한다.
- 사이킷런의 mean_squared_error() 함수로 정확도 측정
	- 실행결과 MSE 값이 높게 나왔으나 이는 다른 예측 모델을 구현하여 함께 비교해볼 필요가 있다.
- 학습 데이터 셋에 대해서도 예측을 수행하고 성능을 평가해보기
	- 실행결과 MSE 값이 테스트 데이터셋을 이용한 성능평가 값과 큰 차이가 없음을 확인할 수 있다.
~~~python
## 7. 성능평가 및 시각화
# 모델 성능 평가 - 테스트 데이터셋
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, pred)
print(mse)

# 학습이 완료된 dtr 객체에서 학습 데이터셋으로 예측 수행
pred = dtr.predict(X_train)

# 모델 성능 평가 - 학습 데이터셋
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_train, pred)
print(mse)
~~~

~~~
6793101269.876856
6684086804.552605
~~~



# 출처

# 관련 노트

# 외부 링크

