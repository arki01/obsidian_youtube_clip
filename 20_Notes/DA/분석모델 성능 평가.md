---
Created: 2024-04-14 16:11
tags: 
aliases:
  - 머신러닝 모델 평가
  - 분류분석 평가지표
  - 성능 평가
  - Evaluation
---

# 개요
- 모형이 내놓은 답과 실제 정답이 어느 정도 일치하는지를 판단하는 것이다.
- 예를들어 스팸 메일인지 아닌지, 환자가 암에 걸렸는지 안 걸렸는지, 은행 대출 승인이 가능한지 불가능한지 등이 있다.
# 내용

#### [[분석모델 성능 평가|분류분석 평가지표]]
##### [[혼동행렬(Confusion Matrix)|혼동행렬]](Confusion Matrix)
###### 개념
- 이진 분류에서 성능 지표로 잘 활용되는 오차행렬
- 이진 분류의 예측 오류가 얼마인지 더불어 어떠한 유형의 예측 오류가 발생하고 있는지 함께 보여준다
- 오차행렬을 통해 정확도, 정밀도, 재현도, F1 Score 등을 구할 수 있다.
~~~ python
# 모델 성능 - 오차 행렬
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, pred)
~~~

~~~
array([[ 9, 0, 0], 
	   [ 0, 10, 0], 
	   [ 0, 2, 9]])
~~~
- 결과값의 가운데 대각선 숫자는 정확하게 분류한 숫자를 보여준다.
- 위 예시의 30개 데이터 중 28개는 정확하게 분류한 것을 알 수 있다.
##### 모델 평가지표
- 오차행렬에 기반한 평가지표는 classification_report() 함수를 통해 구할 수 있다.
~~~ python
# 모델 성능 평가 - 평가지표 계산
from sklearn.metrics import classification_report
rpt = classification_report(y_test, pred)
print(rpt)
~~~

~~~
	          precision recall f1-score support 
           0       1.00   1.00     1.00       9 
           1       0.83   1.00     0.91      10 
           2       1.00   0.82     0.90      11 

    accuracy                       0.93      30 
   macro avg       0.94   0.94     0.94      30 
weighted avg       0.94   0.93     0.93      30
~~~


- 정확도(Accuracy)
	- 실제 데이터와 예측 데이터가 얼마나 정확한지를 판단하는 자료이다.
	- 전체 데이터에서 올바르게 분류한 데이터의 비율을 말한다.
	- 오차행렬 기반으로 계산하며, 사이킷런의 accuracy_score() 함수를 통해 구할 수 있다.
- 정밀도(Precision)
	- Positive로 에측한 것중에서 실제 값이 Positive인 비율을 말한다.
	- 오차행렬을 기반으로 계산하며, 사이킷런의 precision_score() 함수를 통해 구할 수 있다.
- 재현율(Recall),민감도(Sensitivity)
	- 실제 Positive인 값 중 Positive로 분류한 비율을 말한다. (실제 예측와 예측값이 일치)
	- 오차행렬을 기반으로 계산하며, 사이킷런의 recall_score() 함수를 통해 구할 수 있다.
- f1-score 
	- 정밀도와 재현율의 조화평균으로, 정밀도와 재현율 중 한쪽만 클 때보다 두 값이 골고루 클때 큰 값이 된다.
	- 사이킷런의 f1_score() 함수를 통해 구할 수 있다. 
- ![[데이터분석 평가지표_혼동행렬_평가지표.png]]

##### RoC(Receiver Operation Characteristic Curve) Curve
- 거짓긍정비율(FPR)과 참긍정비율(TPR) 간의 관계
- ROC 곡선은 임곗값을 다양하게 조절해 분류 모형의 성능을 비교할 수 있는 그래프로, trade-off 관계인 민감도와 특이도를 기반으로 시각화 한 것이다.
   ![[데이터분석 평가지표__ROC 곡선_AUC개념.png]]

- AUC(Area Under the Curve)
	- ROC 곡선 아래 면적을 AUC(Area Under Curve)라고 하며,
	- AUC가 0.5일 때 분류 능력이 없다고 평가할 수 있고, 면적이 넓을수록, 즉 1에 가까울수록 분류를 잘하는 모형이라 할 수 있다.


#### [[회귀분석 평가지표]]

##### 개요
- 예측값과 실제값의 차이를 기반으로 한 지표들을 이용해 회귀 모형의 성능을 평가할 수 있다.
- 예를 들어 미래의 주식 가격 예측, TV 판매량 예측, 비디오 게임 매출액 예측 등이 있다.
##### 내용
- 평균절대오차(MAE) (Mean Absolute Error)
	- sklearn.metrics.mean_absolute_error
	- 모형의 예측값과 실제값의 차이를 평균한 값으로 정의하며, 음수오차와 양수오차가 서로 상쇄되는 것을 막기 위해 절댓값을 사용한다.
	- 오차의 크기를 그대로 반영하여 오차 평균 크기를 확인
	- 작을수록 좋지만 너무 작으면 과적합 문제가 있음
~~~ python
from sklearn.metrics import mean_absolute_error
mean_absolute_error(df['실제값'], df['예측값'])
~~~~

~~~
3.9294117647058826
~~~


- 평균제곱오차(MSE) (Mean Squared Error)
	- sklearn.metrics.mean_squared_error(squared=True)
	- 모형의 예측값과 실제값 차이를 제곱하여 평균한 값으로 정의한다.
	- 큰 오차를 더 크게, 작은 오차는 더 작게 평가하여 이상치에 민감
~~~ python
from sklearn.metrics import mean_squared_error
mean_squared_error(df['실제값'], df['예측값'])
~~~~

~~~
41.762745098039225
~~~


- 평균제곱근오차(RMSE) (Root Mean Squared Error)
	- sklearn.metrics.mean_squared_error(squared=False)
	- 평균제곱근오차(MSE)에 루트를 씌운 값이다. 회귀모형의 평가지표로 실무에서도 자주 사용된다.
	- MSE 크기를 줄이기 위한 목적으로 사용
~~~ python
from sklearn.metrics import mean_squared_error
mean_squared_error(df['실제값'], df['예측값'], squared=False)
~~~~

~~~
6.462410161699675
~~~


- 평균절대백분율오차(MAPE)
	- 실제값 대비 오차를 평균한 값으로 평균절대오차(MAE)와 같이 절댓값을 모두 더하기 때문에 실제보다 낮은 값으로 예측되는지 높은 값으로 예측되는지 알 수 없다.

- 결정계수 R2 (R-Squared)
	-  sklearn.metrics.r2_score
	- 주어진 데이터에 회귀선이 얼마나 잘 맞는지, 적합 정도를 평가하는 척도이자 독립변수들이 종속변수를 얼마나 잘 설명하는지 보여주는 지표다.
~~~ python
from sklearn.metrics import r2_score
r2_score(df['실제값'], df['예측값'])
~~~~

~~~
0.5145225055729962
~~~




# 출처


# 관련 노트


# 외부 링크

