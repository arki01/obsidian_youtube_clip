---
Created: 2024-05-06 18:06
tags:
---

# 개요


# 내용

###### [[혼동행렬(Confusion Matrix)|혼동행렬]](Confusion Matrix)
- 이진 분류에서 성능 지표로 잘 활용되는 오차행렬
- 이진 분류의 예측 오류가 얼마인지 더불어 어떠한 유형의 예측 오류가 발생하고 있는지 함께 보여준다
- 오차행렬을 통해 정확도, 정밀도, 재현도, F1 Score 등을 구할 수 있다.
~~~ python
# 모델 성능 - 오차 행렬
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, pred)
~~~

~~~
array([[ 9, 0, 0], 
	   [ 0, 10, 0], 
	   [ 0, 2, 9]])
~~~
- 결과값의 가운데 대각선 숫자는 정확하게 분류한 숫자를 보여준다.
- 위 예시의 30개 데이터 중 28개는 정확하게 분류한 것을 알 수 있다.
###### 모델 평가지표
- 오차행렬에 기반한 평가지표는 classification_report() 함수를 통해 구할 수 있다.
~~~ python
# 모델 성능 평가 - 평가지표 계산
from sklearn.metrics import classification_report
rpt = classification_report(y_test, pred)
print(rpt)
~~~

~~~
	          precision recall f1-score support 
           0       1.00   1.00     1.00       9 
           1       0.83   1.00     0.91      10 
           2       1.00   0.82     0.90      11 

    accuracy                       0.93      30 
   macro avg       0.94   0.94     0.94      30 
weighted avg       0.94   0.93     0.93      30
~~~


- 정확도(Accuracy)
	- 실제 데이터와 예측 데이터가 얼마나 정확한지를 판단하는 자료이다.
	- 전체 데이터에서 올바르게 분류한 데이터의 비율을 말한다.
	- 오차행렬 기반으로 계산하며, 사이킷런의 accuracy_score() 함수를 통해 구할 수 있다.
- 정밀도(Precision)
	- Positive로 에측한 것중에서 실제 값이 Positive인 비율을 말한다.
	- 오차행렬을 기반으로 계산하며, 사이킷런의 precision_score() 함수를 통해 구할 수 있다.
- 재현율(Recall),민감도(Sensitivity)
	- 실제 Positive인 값 중 Positive로 분류한 비율을 말한다. (실제 예측와 예측값이 일치)
	- 오차행렬을 기반으로 계산하며, 사이킷런의 recall_score() 함수를 통해 구할 수 있다.
- f1-score 
	- 정밀도와 재현율의 조화평균으로, 정밀도와 재현율 중 한쪽만 클 때보다 두 값이 골고루 클때 큰 값이 된다.
	- 사이킷런의 f1_score() 함수를 통해 구할 수 있다. 
- ![[데이터분석 평가지표_혼동행렬_평가지표.png]]

- RoC(Receiver Operation Characteristic Curve) Curve
	- 거짓긍정비율(FPR)과 참긍정비율(TPR) 간의 관계
	- ROC 곡선은 임곗값을 다양하게 조절해 분류 모형의 성능을 비교할 수 있는 그래프로, trade-off 관계인 민감도와 특이도를 기반으로 시각화 한 것이다.
	   ![[데이터분석 평가지표__ROC 곡선_AUC개념.png]]

- AUC(Area Under the Curve)
	- ROC 곡선 아래 면적을 AUC(Area Under Curve)라고 하며,
	- AUC가 0.5일 때 분류 능력이 없다고 평가할 수 있고, 면적이 넓을수록, 즉 1에 가까울수록 분류를 잘하는 모형이라 할 수 있다.



# 출처


# 관련 노트


# 외부 링크

